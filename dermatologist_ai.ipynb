{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "dermatologist-ai.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aMZ6FYwEEEBY"
      },
      "source": [
        "# Introduction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3COkoSo8EBne"
      },
      "source": [
        "In this notebook, I'm designing an __algorithm that can visually diagnose melanoma, the deadliest form of skin cancer__. In particular, the algorithm will try to distinguish this malignant skin tumor from two types of benign lesions (nevi and seborrheic keratoses).  \n",
        "\n",
        "üìùI also share how I turned my mistakes into __positive learning experiences__! üìù\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wmZkDX12FlZx"
      },
      "source": [
        "# Getting Started\n",
        "\n",
        "This notebook can be easily run on [Google Colab](https://colab.research.google.com/github/sebastienlange/dermatologist-ai/blob/master/dermatologist_ai.ipynb) with GPU activated:  \n",
        "[<img src=\"https://colab.research.google.com/assets/colab-badge.svg\">](https://colab.research.google.com/github/sebastienlange/dermatologist-ai/blob/master/dermatologist_ai.ipynb)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CIqV329tERMn"
      },
      "source": [
        "If you've got any feedback, https://github.com/sebastienlange/dermatologist-ai is the place to leave it. Bug reports, questions, general feedback, and even feature requests are all welcome - just create an [issue](https://github.com/sebastienlange/dermatologist-ai/issues).\n",
        "\n",
        "Want to give your feedback in private? You can email me on lange.sebastien@gmail.com."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rQZtDJckEKNO"
      },
      "source": [
        "![title](https://github.com/sebastienlange/dermatologist-ai/blob/master/images/skin_disease_classes.png?raw=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kAI126NuA8t6"
      },
      "source": [
        "## Settings"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SM-7EAqJEolB",
        "cellView": "form"
      },
      "source": [
        "from IPython.display import HTML\n",
        "\n",
        "def warn_download_images():\n",
        "  return HTML('Please set download_images to True in Getting Started to enable this feature.')\n",
        "  \n",
        "#@markdown Set *download_images* below to True if you want to train or evaluate models yourself, otherwise this notebook can work with downloaded results:\n",
        "download_images = False #@param [\"False\", \"True\"] {type:\"raw\"}\n",
        "\n",
        "#@markdown Set *download_more_images* below to True if you want to download more samples:\n",
        "download_more_images = False #@param [\"False\", \"True\"] {type:\"raw\"}\n",
        "\n",
        "#@markdown Set *mount_gdrive* below to True if you want to mount your own Google Drive (useful to save trained models):\n",
        "mount_gdrive = False #@param [\"False\", \"True\"] {type:\"raw\"}"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QLIXGjiHZU5U"
      },
      "source": [
        "I understood it late, but I finally saved the best-trained models directly on Google Drive. I do not need to think about it anymore and when I reach Google Colab's 12 consecutive hours limit ... I do not just lose my models, which happened to me a few times üòâ"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UfHWjuXE1Ozg"
      },
      "source": [
        "import os\n",
        "\n",
        "google_drive_mount_path = '/content/gdrive'\n",
        "google_drive_shared_path = os.path.join(google_drive_mount_path, 'My Drive' , 'dermatologist-ai')\n",
        "\n",
        "def mount_google_drive():\n",
        "  \n",
        "  from google.colab import drive\n",
        "  drive.mount(google_drive_mount_path)\n",
        "  \n",
        "if mount_gdrive:\n",
        "  mount_google_drive()"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IKdiZf9oCCX6"
      },
      "source": [
        "# Install Prerequisites"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QjJS2jv49Kgv",
        "outputId": "56b38bcb-f5ac-4eaa-ed1d-110a0985c8c6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# google colab does not come with torch installed.\n",
        "!pip install -q torch==1.0.0 \n",
        "\n",
        "# torchvision with twenty-crop \n",
        "!pip install -q git+https://github.com/sebastienlange/vision.git\n",
        "  \n",
        "# Pretrained ConvNets for pytorch: NASNet, ResNeXt, ResNet, InceptionV4, InceptionResnetV2, Xception, DPN, etc.\n",
        "!pip install -q --upgrade pretrainedmodels\n",
        "\n",
        "# display live plots while training (loss, accuracy, ROC AUC...)\n",
        "!pip install -q livelossplot==0.3.3\n",
        "\n",
        "# display execution time for each cell\n",
        "!pip install -q ipython-autotime\n",
        "%load_ext autotime"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  Building wheel for torchvision (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "time: 1.56 ms\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "frkF8-AdIGO6"
      },
      "source": [
        "# Download Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QVgXOpmcjqfk",
        "outputId": "06eaae85-89d0-4191-a47e-4e99e8e4b83e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "phases = ['train', 'valid', 'test']\n",
        "data_root = os.path.join(os.getcwd(), 'data')\n",
        "data_dir = {phase:os.path.join(data_root, phase) for phase in phases}"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "time: 2.65 ms\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pWmJTP9dbPvu",
        "outputId": "852a2026-06d3-4c80-a741-b25264f2d9ca",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "def download(url, destination_folder='.'):\n",
        "  !wget -nc -q --show-progress $url -P $destination_folder\n",
        "\n",
        "if download_images:\n",
        "  # download train.zip, valid.zip, test.zip\n",
        "  for phase in phases:\n",
        "    download(f'https://s3-us-west-1.amazonaws.com/udacity-dlnfd/datasets/skin-cancer/{phase}.zip', data_root)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "time: 3.03 ms\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QPEgl6JFALWA",
        "outputId": "1d3115da-6bc1-453f-98c2-b3ea4c0cdba1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "import zipfile\n",
        "\n",
        "from tqdm import tqdm_notebook as tqdm\n",
        "\n",
        "if download_images:\n",
        "  # unzip train.zip, valid.zip, test.zip\n",
        "  for phase in phases:\n",
        "    if not os.path.exists(data_dir[phase]):\n",
        "      with zipfile.ZipFile(os.path.join(data_root, f'{phase}.zip'), 'r') as myzip:\n",
        "        for file in tqdm(myzip.namelist(), desc=f'Extracting {phase}.zip'):\n",
        "          myzip.extract(member=file, path=data_root)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "time: 24.2 ms\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OOPzKKdrCxOq"
      },
      "source": [
        "# Explore Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VPXGJHppJ4Tp",
        "outputId": "8fe00d9e-18bf-4edb-f415-22ba87c6fed1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "import glob\n",
        "\n",
        "classes = [path.split(os.path.sep)[-1] for path in sorted(glob.glob(os.path.join(data_root, 'train', '*')))] if download_images else ['melanoma', 'nevus', 'seborrheic_keratosis']\n",
        "classes"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['melanoma', 'nevus', 'seborrheic_keratosis']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        },
        {
          "output_type": "stream",
          "text": [
            "time: 6.2 ms\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K6vBvyVjb7hs"
      },
      "source": [
        "These are our 3 classes! Let's now explore how much data we have and how it's distributed:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nOOuwRXUJ-aG",
        "outputId": "42fcc96b-d5af-4492-ce7a-674bfee38543",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "import seaborn as sns\n",
        "import warnings\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "def print_images_distribution(plot=False):\n",
        "  \n",
        "  if not download_images:\n",
        "    return warn_download_images()\n",
        "\n",
        "  image_repartition = pd.DataFrame(index=[d.split(os.path.sep)[-1] for d in data_dir.values()], columns=classes)\n",
        "  plot_data = pd.DataFrame(columns=['Class', 'Phase', 'Count'])\n",
        "  \n",
        "  for phase in [d.split(os.path.sep)[-1] for d in data_dir.values()]:\n",
        "    for disease in classes:\n",
        "      count =  len(glob.glob(os.path.join(data_root, phase, disease, '*.jpg')))\n",
        "      image_repartition.loc[phase][disease] = count\n",
        "      plot_data = plot_data.append({'Class': disease, 'Phase': phase, 'Count': count}, ignore_index=True)\n",
        "\n",
        "  image_repartition.loc['TOTAL'] = image_repartition.sum(axis=0)          \n",
        "  image_repartition['TOTAL'] = image_repartition.sum(axis=1).astype(int)\n",
        "  image_repartition['Ratio'] = np.round(image_repartition.TOTAL / (image_repartition.TOTAL.sum() - image_repartition.TOTAL[-1]), 2)\n",
        "  \n",
        "  if plot:\n",
        "    sns.set_style(\"whitegrid\")\n",
        "    sns.barplot(x='Class', y='Count', hue='Phase', data=plot_data, palette='Blues')\n",
        "\n",
        "  return image_repartition"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "time: 290 ms\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Leax1tN8haR9",
        "outputId": "1a672f0c-f41c-4146-8230-7c537ba360dd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "print_images_distribution(True)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "Please set download_images to True in Getting Started to enable this feature."
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        },
        {
          "output_type": "stream",
          "text": [
            "time: 2.83 ms\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TGw8rgnbOVwE"
      },
      "source": [
        "## Class imbalance\n",
        "üìùWe have a __class imbalance problem__ in this dataset: __nevus is much more represented__ whereas we are interested to have a great accuracy in the prediction of melanoma and seborrheic_keratosis.  \n",
        "\n",
        "Without any change, the model would just learn to predict nevus. (My first learning experience) üìù"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ft3BXJeGKCry"
      },
      "source": [
        "## Download more data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hqjo456tj3re"
      },
      "source": [
        "Fortunately, we can __download more samples__ from [ISIC Archive](https://www.isic-archive.com/#!/onlyHeaderTop/gallery).  \n",
        "\n",
        "So let's download more samples and ensure we don't have any duplicate!\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IDAT3_s51y7f",
        "outputId": "c3be0ea0-b75f-411a-dfd4-921bc753588a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "if download_more_images:\n",
        "  data_dir['more'] = os.path.join(data_root, 'more')"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "time: 1.16 ms\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KgJisNzHQmKg"
      },
      "source": [
        "The additional data is stored publicly on my personal Google Drive."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B1VD19Tvw5TC",
        "outputId": "bf21389d-b350-4a98-959e-ba1ab6279e8c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "google_drive_download_link = 'https://drive.google.com/uc?export=download'\n",
        "\n",
        "additional_image_files = {\n",
        "                          # additional images downloaded from https://www.isic-archive.com/#!/onlyHeaderTop/gallery\n",
        "                          'melanoma-1.zip': '10Wo3RwSC-O4YEAZkU77ZehciDpWCn3Si',\n",
        "                          'melanoma-2.zip': '1L9XLBX1sIYX9GiEdfhWizeOuP52Pdgm2',\n",
        "                          'melanoma-3.zip': '1jP7YCh5e2g588f0j5ikURsU7oNCLtQCL',\n",
        "                          'melanoma-4.zip': '1_IjF2lZZE2yeaZSE9rXqrfrgXd_UdKZi',\n",
        "                          'melanoma-5.zip': '1KRF0zRWU-yGSa87p04kQvCHvTiCnZnOO',\n",
        "                          'melanoma-6.zip': '1vL18GGPxNCY--R-VLoFRqxeI2ZiSfTJi',\n",
        "                          'melanoma-7.zip': '1bFoqLrcUu1YwYq5EQxMrg7WQjRRHNA8U',\n",
        "                          'melanoma-8.zip': '1tFW8OkvpWyXpm0ueeCld0qG5k6uk7q3X',\n",
        "                          'nevus-1.zip': '1GqBasxjZJNh_C8B4uWaxGLbvDU3HE0e3',\n",
        "                          'nevus-2.zip': '1H7PDGPnCapqwmYY2FYaSp1ppqMg9W72a',\n",
        "                          'nevus-3.zip': '1snmb9zMdcAjVxW9sRpduMbP9gf3gUqGh',\n",
        "                          'nevus-4.zip': '1KFDlx1W8AC97hzTRvVvQJ29EoUiH78pQ',\n",
        "                          'seborrheic_keratosis.zip': '1wTgY_AFo0wLF-0g-2mxzkwAv2EXmpGq1',\n",
        "}\n",
        "\n",
        "# my own pretrained models\n",
        "trained_models_files = {               \n",
        "    'DenseNet.pt': '1c83tG5-fvTQsRB-N-ADLXwG7_W3mbiwr',\n",
        "    \n",
        "    'Inception3_3_0.9051.pt': '1-4-NBGjBzaVLbdKyQYU35JsG7hH9dmpD',\n",
        "    'Inception3_3_0.9089.pt': '1-BE3K5jyDZ21rsFpIG_22GfrCvriLM_G',\n",
        "    'Inception3_1_Mixed_5c.branch3x3dbl_1.bn_0.9126.pt': '1-nDIjuT0FY4UubhM35XCnfAoCVfJD1JI',\n",
        "          \n",
        "    'NASNetALarge.pt': '144_os2xtFyN9RDH-1cYiXfRd1p5hA-Gn',\n",
        "    'NASNetALarge_4_0.9106.pt': '1JJLyVraQc8ojllTdU4LyyVo5mkpFtq-3',\n",
        "    \n",
        "    #'Xception_1_block4_0.9076.pt': '1WYvzipY-pR5Db6b2bkI8BDfv9HLc1PNt',\n",
        "    'Xception_1_block4_0.9120.pt': '1308ZVjdOsMe_voKRQ5wdejMapa01g1cK',\n",
        "    'Xception_2_block3.rep.4.conv1_0.9185.pt': '1-mjjS7Hgm4knFp5W7XCY3t9DCwCHSAm1',\n",
        "    \n",
        "    'SENet_3_layer2.1_0.8988.pt': '1--Q3L7DkuHL3Qr8A6gJZIcP1FiOiMQm_',\n",
        "    \n",
        "    'InceptionV4_2_features.6.branch1.0_0.9117.pt': '1-0EXqfAEqO_sqeWW3X7Po6-yRbbVkVyu',\n",
        "    #'InceptionV4_3_features.7_0.9057.pt': '',\n",
        "    #'InceptionV4_6_features.6.branch2_0.8986.pt': '',\n",
        "    'InceptionV4_4_features.6.branch2.1.bn_0.9152.pt': '1-5caaGcjEZEm-FVfGvL-J1imdRR52OOy',\n",
        "    \n",
        "    'InceptionResNetV2_2_repeat.0.branch2_0.8966.pt': '1lMsnmDJFoT-xTeUM6UC3axTSaL5f9F0K',\n",
        "    \n",
        "    #'PNASNet5Large_1_cell_1.comb_iter_4_left_0.9058.pt': '10IM9Pp5a1T8jdxjPKnzqKMYZKi6kbtOL',\n",
        "    #'PNASNet5Large_0_cell_1_0.9070.pt': '',\n",
        "    'PNASNet5Large_3_cell_0.comb_iter_4_left.separable_2_0.9254.pt': '17OhGCpKNloXs8Oo6UJOT6g3HGKWE421-',\n",
        "}"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "time: 13.5 ms\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EGmQ7vWc_IDG"
      },
      "source": [
        "Google requires a confirmation because it can not perform virus scan on big files. So the download_file function below is a little bit tricky because I need to get the confirm value.  \n",
        "Otherwise files should be downloaded manually."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lcOOPRWchB7q",
        "outputId": "f181c235-5f34-480b-e0b5-d2916b4978a2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "import html\n",
        "\n",
        "# download files on google drive and bypass virus scan confirm if any... hence the many lines of codes\n",
        "def download_file(fn, quiet=True):\n",
        "  \n",
        "  if not os.path.exists(fn) and not os.path.exists(os.path.join(google_drive_shared_path, fn.split(os.path.sep)[-1])):\n",
        "    files_available_for_download = additional_image_files if os.path.splitext(fn)[1]=='.zip' else trained_models_files\n",
        "    path = os.path.dirname(fn)\n",
        "    print(f'Downloading {os.path.basename(fn)} with id {files_available_for_download[os.path.basename(fn)]} to path {path}')\n",
        "    url = f'\"{google_drive_download_link}&id={files_available_for_download[os.path.basename(fn)]}\"'\n",
        "    temp_cookie = 'gdown.cookie.temp'\n",
        "    quiet = '-q' if quiet else ''\n",
        "    !wget $quiet --progress=dot:giga --no-check-certificate --load-cookie $temp_cookie --save-cookie $temp_cookie $url -O $fn\n",
        "    \n",
        "    confirm = None\n",
        "    confirm_word = '&confirm='\n",
        "    id_word = '&id='\n",
        "    confirm_word_escaped = html.escape(confirm_word)\n",
        "    id_word_escaped = html.escape(id_word)\n",
        "    \n",
        "    while os.path.getsize(fn) < 100000:\n",
        "      for i, line in enumerate(open(fn)):\n",
        "        if i<5 and confirm_word_escaped in line: \n",
        "          # it's not the expected file but a redirection to confirm download\n",
        "          start_idx = line.find(confirm_word_escaped) + len(confirm_word_escaped)\n",
        "          end_idx = line.find(id_word_escaped, start_idx)\n",
        "          \n",
        "          # get confirm value\n",
        "          confirm=line[start_idx:end_idx]\n",
        "        \n",
        "      if confirm is not None:\n",
        "        # Google requires no virus scan confirm\n",
        "        url = url.replace(id_word, f'{confirm_word}{confirm}{id_word}')\n",
        "        !wget $quiet --progress=dot:giga --no-check-certificate --load-cookie $temp_cookie --save-cookie $temp_cookie $url -O $fn\n",
        "    \n",
        "    !rm -r $temp_cookie"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "time: 35.3 ms\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5KpstZQ1GaQ4",
        "outputId": "b2b9c7a1-ef7a-4f3b-9539-38ddc436e94e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "if download_images and download_more_images:\n",
        "  # download all zip files containing more images\n",
        "  for fn in additional_image_files.keys():\n",
        "    if os.path.splitext(fn)[1]=='.zip':\n",
        "      download_file(os.path.join(data_root, fn))"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "time: 2.06 ms\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PLhAV_8pAreY"
      },
      "source": [
        "As you will see when extracting images, I have unintentionally downloaded duplicates! The code below prevents any duplication."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PyjJ0tw_dOOE",
        "outputId": "7d039e91-104d-4320-a7f5-91842345e13d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "def get_unique_images(myzip):\n",
        "  # prevent having any duplicate accross train, valid and test!!!\n",
        "  \n",
        "  for image_file in myzip.namelist():\n",
        "    duplicate = False\n",
        "    for d in data_dir.values():\n",
        "      if os.path.exists(f'{d}/{disease}/{image_file}'):\n",
        "        duplicate = True\n",
        "    if not duplicate:\n",
        "      yield image_file"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "time: 3.62 ms\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vsp6MGCEILIK",
        "outputId": "abd9c3fb-977c-4c2b-b060-9b68a34077be",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "if download_images and download_more_images:\n",
        "  # extract images from zip files\n",
        "  for disease in classes:\n",
        "    for root_path in [data_root, google_drive_shared_path]:\n",
        "      for fn in sorted(glob.glob(f'{root_path}/{disease}*.zip')):\n",
        "        with zipfile.ZipFile(fn, 'r') as myzip:\n",
        "          members = list(get_unique_images(myzip))\n",
        "          print(f'Extracting {len(members)} images from {fn.split(os.path.sep)[-1]} and ignoring {len(myzip.namelist())-len(members)} duplicates')\n",
        "\n",
        "          for file in members:\n",
        "            myzip.extract(member=file, path=os.path.join(data_dir['more'], disease))"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "time: 5.71 ms\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KK2JPBOliv31",
        "outputId": "c96fdfb7-6465-4f60-be66-f77176499de7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "print_images_distribution()"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "Please set download_images to True in Getting Started to enable this feature."
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        },
        {
          "output_type": "stream",
          "text": [
            "time: 4.3 ms\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bog8SqysIYTc"
      },
      "source": [
        "## Oversampling"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c3B9B8NWoXuc"
      },
      "source": [
        "We now have enough melanoma samples.  \n",
        "But __still not enough seborrheic keratosis__. So I choose the __oversampling approach__ for this unbalanced class and I increase the number of observations which are just copies of existing samples.  \n",
        "\n",
        "At each epoch, for a given phase, I duplicate image indexes randomly until I have as many samples in each class."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-_0h60SOIWAe",
        "outputId": "a5239cff-8828-4261-c5f6-f0b26d7e2c17",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "import random\n",
        "\n",
        "def balance_indices(epoch_indices, y, subset=1):\n",
        "  # it uses oversampling to increase the size of imbalanced classes by randomly selecting more samples until all classes have same images count\n",
        "  # subset allows a \"cold\" start: reusing a portion of images of most represented class\n",
        "  \n",
        "  images_per_class = {i:[] for i in range(len(classes))}\n",
        "          \n",
        "  for i in epoch_indices:\n",
        "    images_per_class[y[i]].append(i)\n",
        "  \n",
        "  counts = [len(indexes) for indexes in images_per_class.values()] \n",
        "  max_samples_per_class = int(np.max(counts) * subset)\n",
        "  if subset < 1:\n",
        "    logging.info(f'Cold start: reusing {subset*100:.1f}% of images of most represented class')\n",
        "  \n",
        "  for i in range(len(classes)):\n",
        "    if len(images_per_class[i]) > max_samples_per_class:\n",
        "      images_per_class[i] = random.sample(images_per_class[i], max_samples_per_class)\n",
        "    else:\n",
        "      for j in range(max_samples_per_class-len(images_per_class[i])):\n",
        "        over_sampled_i = np.random.randint(0, counts[i])\n",
        "        images_per_class[i].append(images_per_class[i][over_sampled_i])\n",
        "      \n",
        "  return sorted([item for sublist in images_per_class.values() for item in sublist])"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "time: 17.8 ms\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RKoC1rNyJDi-"
      },
      "source": [
        "## Too small validation set\n",
        "üìù In addition, the initial validation set is very small (150 samples) and will not be a good predictor of model performance. (another learning experience) üìù\n",
        "\n",
        "So I __change the partition to have 80% in training (3412 samples) an 20% in validation (853)__.  \n",
        "\n",
        "I of course keep the test set imbalanced for the challenge purpose."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6HRl1JIkEcXM"
      },
      "source": [
        "# Data Augmentation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IEx5Duk0EqnY"
      },
      "source": [
        "As there are \"only\" 4265 images in the training set, one way to enlarge the dataset is to \"augmentate\" the images by applying different chained transformations.   \n",
        "\n",
        "I create two chained transformations: \n",
        " - one for validation and test with image resized to 133% of the size required by the model (224, 299, or 331). And finally images are normalized;\n",
        " - one for training with some data augmentation: \n",
        "     - __random image rotation__ with a maximum of 30 degrees, \n",
        "     - __random resized crop__ to the same size as above, with a 0.70-1.00 scale, \n",
        "     - __random translation__ with a maximum absolute fraction of 20%, \n",
        "     - __random zoom__ with a 0.8-1.2 scale,\n",
        "     - __random Gaussian noise__ from [imgaug](https://github.com/aleju/imgaug),\n",
        "     - __random horizontal/vertical flip__.\n",
        "     - __random color jitter__,\n",
        "     - and __random Gaussian blur__ from [imgaug](https://github.com/aleju/imgaug),\n",
        " \n",
        "These transformations will also __reduce overfitting to the training set__. \n",
        "\n",
        "Color jitter and particularly Gaussian blur were not present in all of my runs because it makes learning much slower, while it doesn't seem to reduce overfitting as much as I would like."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zbC4wFxULDWr"
      },
      "source": [
        "The ImgAugTransform class below is a wrapper around [imgaug](https://github.com/aleju/imgaug) to make it compatible with [PyTorch transforms](https://pytorch.org/docs/stable/torchvision/transforms.html).  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Z1dtu-wVSDs",
        "outputId": "c86c3f2d-4f64-40e3-80a0-f9ffe11b9ac8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "import imgaug as ia\n",
        "\n",
        "from imgaug import augmenters as iaa\n",
        "\n",
        "# wrapper around imgaug compatible with pytorch transforms\n",
        "class ImgAugTransform:\n",
        "  def __init__(self, aug):\n",
        "    self.aug = aug\n",
        "      \n",
        "  def __call__(self, img):\n",
        "    img = np.array(img)\n",
        "    return Image.fromarray(np.uint8(self.aug.augment_image(img)))\n",
        "  \n",
        "  def __repr__(self):\n",
        "    return self.__class__.__name__ + ' ' + str(self.aug)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "time: 199 ms\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iRCk6P3TEK7W"
      },
      "source": [
        "I noticed that dermascopic images sometimes include hairs, or some kind of colored patch, or a ruler.  \n",
        " \n",
        "So in one of my last runs, I quickly created and added __my own augmenter to draw random fake hairs__:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I7FnzyAVETTZ",
        "outputId": "54160023-2917-4362-e215-1a72c3ba25f4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "from PIL import Image, ImageDraw\n",
        "\n",
        "# draw random lines for fake hairs\n",
        "class DrawHair:\n",
        "\n",
        "  def __call__(self, img):\n",
        "\n",
        "    draw = ImageDraw.Draw(img)\n",
        "\n",
        "    for i in range(np.random.randint(5,20)):\n",
        "        draw.line((np.random.randint(0,img.size[0]),\n",
        "                   np.random.randint(0,img.size[1]),\n",
        "                   np.random.randint(0,img.size[0]),\n",
        "                   np.random.randint(0,img.size[1])), fill=0, width=np.random.randint(1,7))\n",
        "    del draw\n",
        "\n",
        "    return img"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "time: 54.2 ms\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j1ILsywOEoms"
      },
      "source": [
        "And my own augmenter to draw __random fake colored elliptic or rectangular patch__:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j3LR4UWiEnNc",
        "outputId": "ccf14431-9e7d-40ae-84c4-efcc34509955",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# draw random colored rectangles or ellipse for fake \"patches\"\n",
        "class DrawShape:\n",
        "\n",
        "  def __call__(self, img):\n",
        "\n",
        "    draw = ImageDraw.Draw(img)\n",
        "\n",
        "    corner = np.random.randint(1,5)\n",
        "    coord = ((np.random.randint(0,img.size[0]/8) + img.size[0]/2* (corner==2 or corner==4), np.random.randint(0,img.size[1]/8 + img.size[0]/2* (corner==3 or corner==4))), \n",
        "                    (np.random.randint(0,img.size[0]/2) + img.size[0]/2* (corner==2 or corner==4), np.random.randint(0,img.size[1]/2 + img.size[0]/2* (corner==3 or corner==4))))\n",
        "    if np.random.rand()<0.5:\n",
        "        draw.ellipse(coord, fill=np.random.randint(0,240))\n",
        "    else:\n",
        "        draw.rectangle(coord, fill=np.random.randint(0,240))\n",
        "\n",
        "    del draw\n",
        "\n",
        "    return img"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "time: 22.7 ms\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YiD9mtRBFRLE"
      },
      "source": [
        "Not all models require the same input image size. Here are the ones I needed:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vqpWjxP2ooN_",
        "outputId": "078b27e2-9566-4cb7-c8e4-6a263aff14ef",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "def get_image_size(model_name):\n",
        "  \n",
        "  model_name = model_name.lower()\n",
        "  \n",
        "  if model_name in ['inception3', 'inceptionv4', 'xception', 'inceptionresnetv2']:\n",
        "    return 299\n",
        "  elif model_name in ['nasnetalarge', 'pnasnet5large', 'polynet']:\n",
        "    return 331\n",
        "  else:\n",
        "    return 224"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "time: 3.54 ms\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "blaso4OYF16L"
      },
      "source": [
        "So here are my transforms:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "97tRv0ZZONxb",
        "outputId": "93f1c29a-7430-4628-9611-799f7529ca9f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# !pip uninstall pillow -y &&  CC=\"cc -mavx2\" pip install -U --force-reinstall pillow-simd"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Uninstalling Pillow-8.0.1:\n",
            "  Successfully uninstalled Pillow-8.0.1\n",
            "Processing /root/.cache/pip/wheels/d3/ac/4f/4cdf8febba528e5f1b09fc58d5181e1c12ed1e8655dcd583b8/Pillow_SIMD-7.0.0.post3-cp36-cp36m-linux_x86_64.whl\n",
            "Installing collected packages: pillow-simd\n",
            "  Found existing installation: Pillow-SIMD 7.0.0.post3\n",
            "    Uninstalling Pillow-SIMD-7.0.0.post3:\n",
            "      Successfully uninstalled Pillow-SIMD-7.0.0.post3\n",
            "Successfully installed pillow-simd-7.0.0.post3\n",
            "time: 3.24 s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NA9zMP2eRWSp",
        "outputId": "7a67e102-3c4b-441d-98dc-f5a2367b5cc2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 230
        }
      },
      "source": [
        "!pip uninstall PIL -y\n",
        "!pip uninstall Pillow -y\n",
        "!pip install Pillow"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[33mWARNING: Skipping PIL as it is not installed.\u001b[0m\n",
            "\u001b[33mWARNING: Skipping Pillow as it is not installed.\u001b[0m\n",
            "Collecting Pillow\n",
            "  Using cached https://files.pythonhosted.org/packages/5f/19/d4c25111d36163698396f93c363114cf1cddbacb24744f6612f25b6aa3d0/Pillow-8.0.1-cp36-cp36m-manylinux1_x86_64.whl\n",
            "\u001b[31mERROR: albumentations 0.1.12 has requirement imgaug<0.2.7,>=0.2.5, but you'll have imgaug 0.2.9 which is incompatible.\u001b[0m\n",
            "Installing collected packages: Pillow\n",
            "Successfully installed Pillow-8.0.1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "PIL"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "time: 4.58 s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vTM_xwSGE5Up",
        "outputId": "96073faf-8e35-498a-931b-66df46435d49",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "import torchvision.transforms as transforms\n",
        "\n",
        "def get_transform(model_name, phase, resize=0.75, n_crops=5):\n",
        "\n",
        "  image_size = get_image_size(model_name)\n",
        "\n",
        "  if phase=='train':\n",
        "    return transforms.Compose([\n",
        "                                transforms.RandomResizedCrop(image_size, scale=(0.7,1)),\n",
        "                                transforms.RandomApply([DrawHair()], p=0.25),\n",
        "                                transforms.RandomApply([DrawShape()], p=0.25),\n",
        "                                transforms.RandomAffine(degrees=30, translate=(0.2,0.2), scale=(0.8,1.2), shear=(-8,8)),\n",
        "                                transforms.ColorJitter(brightness=0.25, contrast=0.25, saturation=0.25, hue=0.1),\n",
        "                                ImgAugTransform(iaa.Sometimes(0.2, iaa.AdditiveGaussianNoise(loc=0, scale=(0.0, 0.15*255), per_channel=0.5, name='GaussianNoise'))),\n",
        "                                ImgAugTransform(iaa.Sometimes(0.2, iaa.GaussianBlur(sigma=(0, 1.0), name='GaussianBlur'))),\n",
        "                                ImgAugTransform(iaa.Sometimes(0.2, iaa.CoarseDropout((0.03, 0.15), size_percent=(0.05, 0.2), per_channel=0.2))),\n",
        "                                ImgAugTransform(iaa.Sometimes(0.2, iaa.JpegCompression(0.87))),\n",
        "                                #ImgAugTransform(iaa.Sometimes(0.2, iaa.Sharpen(alpha=(0, 1.0), lightness=(0.75, 1.5), name='Sharpen'))),\n",
        "                                transforms.RandomHorizontalFlip(),\n",
        "                                transforms.RandomVerticalFlip(),\n",
        "                                transforms.ToTensor(),\n",
        "                                get_normalize(model_name)])\n",
        "  elif n_crops==1:    \n",
        "    return transforms.Compose([\n",
        "                                transforms.Resize(int(image_size/resize)),\n",
        "                                transforms.CenterCrop(image_size),\n",
        "                                transforms.ToTensor(),\n",
        "                                get_normalize(model_name)])\n",
        "  else:\n",
        "    return transforms.Compose([\n",
        "                                transforms.Resize(int(image_size/resize)),\n",
        "                                MultiCrop(n_crops, image_size),\n",
        "                                transforms.Lambda(lambda crops: torch.stack([transforms.ToTensor()(crop) for crop in crops])),\n",
        "                                transforms.Lambda(lambda crops: torch.stack([get_normalize(model_name)(crop) for crop in crops]))\n",
        "    ])\n",
        "  \n",
        "# All pre-trained models expect input images normalized in the same way,\n",
        "# i.e. mini-batches of 3-channel RGB images of shape (3 x H x W),\n",
        "# where H and W are expected to be at least 224 (299 for inception)\n",
        "\n",
        "def get_normalize(model_name):\n",
        "  \n",
        "  if model_name.lower() in ['inceptionv4', 'pnasnet5large', 'inceptionresnetv2', 'xception']: #'NasNetALarge']\n",
        "    return transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])  \n",
        "    # My NasNetALarge models were erroneoously trained with values below (rather than above) so I did not fix my bug otherwise I had to train it again\n",
        "    \n",
        "  return transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "time: 302 ms\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a2Tv1HWfOJSt"
      },
      "source": [
        "## Multi-crop"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aSLSiCvOOino"
      },
      "source": [
        "To multi-crop is to crop the input image into multi sub-images, input into network for classification, and average the result to increase the accuracy. PyTorch provides two multi-crop classes:\n",
        "- [FiveCrop](https://pytorch.org/docs/stable/torchvision/transforms.html#torchvision.transforms.TenCrop) crops the given image into four corners and the central crop;\n",
        "- and [TenCrop](https://pytorch.org/docs/stable/torchvision/transforms.html#torchvision.transforms.TenCrop) crops the given image into four corners and the central crop plus the flipped version of these ((horizontal flipping is used by default).  \n",
        "\n",
        "You may have noticed in the code above the MultiCrop function. Here it is:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lSOKnHB-O9Hm",
        "outputId": "be6baa83-57ec-47ef-f718-ba139db481e3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "def MultiCrop(n_crops, image_size):\n",
        "\n",
        "  if n_crops == 5:\n",
        "    return transforms.FiveCrop(image_size)\n",
        "  elif n_crops == 10:\n",
        "    return transforms.TenCrop(image_size)\n",
        "  elif n_crops == 20:\n",
        "    return transforms.TwentyCrop(image_size)\n",
        "  else:\n",
        "    raise ValueError(f'Unsupported n_crops: {n_crops}')"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "time: 2.16 ms\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1WPIrXGrLqiT"
      },
      "source": [
        "As said earlier, [TenCrop](https://pytorch.org/docs/stable/torchvision/transforms.html#torchvision.transforms.TenCrop) flips image vertically __OR__ horizontally.  \n",
        "\n",
        "So I created __TwentyCrop__ hereafter to flip image both vertically __AND__ horizontally. üí•"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y_bM8JyctPlv",
        "outputId": "68ee0528-71ef-420c-c02e-27be3cde9f89",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "import numbers\n",
        "\n",
        "from torchvision.transforms import functional as F\n",
        "\n",
        "class TwentyCrop(object):\n",
        "  \"\"\" TenCrop flips image vertically or horizontally. TwentyCrop does both.\n",
        "  \"\"\"\n",
        "\n",
        "  def __init__(self, size):\n",
        "      self.size = size\n",
        "      if isinstance(size, numbers.Number):\n",
        "          self.size = (int(size), int(size))\n",
        "      else:\n",
        "          assert len(size) == 2, \"Please provide only two dimensions (h, w) for size.\"\n",
        "          self.size = size\n",
        "\n",
        "  def __call__(self, img):\n",
        "\n",
        "    first_ten = F.ten_crop(img, self.size, vertical_flip=False)\n",
        "    img = F.vflip(img)\n",
        "    second_ten = F.ten_crop(img, self.size, vertical_flip=False)\n",
        "\n",
        "    return first_ten + second_ten\n",
        "\n",
        "  def __repr__(self):\n",
        "      return self.__class__.__name__ + '(size={0})'.format(self.size)"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "time: 10 ms\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Dyy43PoNHn8"
      },
      "source": [
        "üìùMulti-crop takes as many times as there are crops, so of course it can be very slow! üò¥  \n",
        "\n",
        "So I enabled it only when metrics reach a predefined threshold as shown in code below. üìù"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qZkuyI_RK35Y",
        "outputId": "5f1ad6b5-eab2-46fa-ec44-4a1d1967ebae",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "def get_n_crops(is_test_phase, metric, n_crops_stages={5: 0.875, 10: 0.890}):\n",
        "\n",
        "  if is_test_phase:\n",
        "    for crops, threshold in reversed(sorted(n_crops_stages.items())): \n",
        "      if metric >= threshold:\n",
        "        return crops\n",
        "\n",
        "  return 1"
      ],
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "time: 2.21 ms\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7ROasS8uKd6b"
      },
      "source": [
        "## Image preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sZN30-8OBX9g"
      },
      "source": [
        "A lot of images are very big (5-20 MB, width/height of 4000-6000 pixels).  \n",
        "\n",
        "üìùLoading and resizing them takes around 90% of the time of each epoch! üò¥ Let's create copies with a smaller size still compatible with our transforms and some models requiring varying input size, so that __the duration of each epoch is reduced by 90%__! üí•\n",
        "\n",
        "I would have saved a lot of time if I realized it earlier üìù"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EwKzDFTEmx_C",
        "outputId": "b0efffa8-151e-4cad-a452-7caa0d21514a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "def preprocess_images(folder, force=False):\n",
        "  \n",
        "  if force or folder.endswith('_resized') and not os.path.exists(folder):\n",
        "    print('For performance purpose, images pre-processing (resizing) is required...')\n",
        "    \n",
        "    for dir_name, path in data_dir.items():\n",
        "      for fn in tqdm(sorted(glob.glob(os.path.join(path, '*', '*.jpg'))), desc=('Resize ' + path.split(os.path.sep)[-1])):\n",
        "        resized_fn = fn.replace(dir_name, dir_name + '_resized')\n",
        "        if not os.path.exists(resized_fn):\n",
        "          if not os.path.exists(os.path.dirname(resized_fn)): \n",
        "            os.makedirs(os.path.dirname(resized_fn))\n",
        "          try:\n",
        "            im = Image.open(fn)\n",
        "            # double size required by largest tested model (NASNetALarge)\n",
        "            size = 331*2, 331*2\n",
        "            if im.size != size:\n",
        "              im = im.resize(size, Image.ANTIALIAS)\n",
        "            im.save(resized_fn)\n",
        "          except OSError as e:\n",
        "            print(\"error \" + str(e))"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "time: 12 ms\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qXIs9miUFNsq"
      },
      "source": [
        "# Load Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N-hNWYMHFW2h"
      },
      "source": [
        "Three different data loaders are initialized for the three different sets of images: training, validation, and test.  \n",
        "For each iteration, each data loader will return a batch with multiple images and perform one step of forward and back propagation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-02-14T07:53:12.455142Z",
          "start_time": "2019-02-14T07:53:12.424192Z"
        },
        "id": "PkpbePrj_V8s",
        "outputId": "3a2d08d5-8254-429e-f70c-ae9a2d4fd986",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "import torch\n",
        "\n",
        "from torchvision import datasets\n",
        "from PIL import ImageFile\n",
        "\n",
        "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
        "\n",
        "# number of subprocesses to use for data loading\n",
        "num_workers = 0\n",
        "\n",
        "# how many samples per batch to load\n",
        "batch_size = {phase: 32 if phase=='train' else 16 for phase in phases}\n",
        "\n",
        "if download_images:\n",
        "  image_folders = {phase: datasets.ImageFolder(data_dir[phase], transform=get_transform('densenet', phase)) for phase in phases}\n",
        "  loaders = {phase: torch.utils.data.DataLoader(image_folders[phase], batch_size=batch_size[phase], num_workers=num_workers, shuffle=(phase=='train')) for phase in phases}"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "time: 7.35 ms\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lyAXS7BsjBgr"
      },
      "source": [
        "## Batch samples with and without data augmentation\n",
        "Let's show a single batch from the __train set__ with __data augmentation__:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-02-14T07:53:21.516456Z",
          "start_time": "2019-02-14T07:53:17.951231Z"
        },
        "id": "-K40u3HY_V81",
        "outputId": "af8ca4b0-c57d-4aa3-ab1f-496b5e1e743c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "import torchvision\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "def imshow(img):\n",
        "    img = img / 2 + 0.5     # unnormalize\n",
        "    npimg = img.numpy()\n",
        "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
        "    \n",
        "def show_batch(phase, batch_size, nrow):\n",
        "  \n",
        "  if not download_images:\n",
        "    return warn_download_images()\n",
        "  \n",
        "  data_loader = torch.utils.data.DataLoader(image_folders[phase], batch_size=batch_size, num_workers=num_workers, shuffle=(phase=='train'))\n",
        "  dataiter = iter(data_loader)\n",
        "  images, labels = dataiter.next()\n",
        "  \n",
        "  if len(images.size()) == 5:\n",
        "    # re-transform multi-crop as 4D tensor for vizualisation\n",
        "    bs, ncrops, c, h, w = images.size()  \n",
        "    images = images.view(-1, c, h, w)\n",
        "  \n",
        "  fig = plt.figure(figsize=(12, 8))\n",
        "  imshow(torchvision.utils.make_grid(images, nrow, padding=10))\n",
        "  \n",
        "show_batch('train', batch_size=32, nrow=8)"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "Please set download_images to True in Getting Started to enable this feature."
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        },
        {
          "output_type": "stream",
          "text": [
            "time: 21.3 ms\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ss2NBtLxjRfZ"
      },
      "source": [
        "And a single batch from the __test set__ with __NO data augmentation__ but with __5-crop__:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-zdiw_23glJa",
        "outputId": "6274c24d-3d20-426c-acd9-7c8416694920",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "# shows batch_size * n_crops images\n",
        "show_batch('test', batch_size=5, nrow=5)"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "Please set download_images to True in Getting Started to enable this feature."
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        },
        {
          "output_type": "stream",
          "text": [
            "time: 3.83 ms\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X_falmAa_oxq"
      },
      "source": [
        "# Create a Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "odh1xqUDPPD0"
      },
      "source": [
        "Let's first detect if we have a GPU available:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-02-14T07:53:32.969017Z",
          "start_time": "2019-02-14T07:53:32.963985Z"
        },
        "id": "qkJTmq-s_V9G",
        "outputId": "b8bbd33d-6c11-4e46-dca1-54f8c264e010",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "time: 63.2 ms\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "593OHQn4AUsD"
      },
      "source": [
        "Having satisfactory performance with a model created from scratch would take days or weeks to train... and still you have to train multiples architectures and pick the best one.  \n",
        "\n",
        "__[Transfer Learning](https://cs231n.github.io/transfer-learning/)__ is the reuse of a pre-trained model on a new problem. It is currently very popular in the field of Deep Learning because it enables you to train Deep Neural Networks with comparatively little data.\n",
        "There are a some pre-trained Machine Learning models out there that became quite popular. One of them is the [DenseNet-161](https://arxiv.org/pdf/1608.06993.pdf) model, which was trained for the ImageNet ‚ÄúLarge Visual Recognition Challenge‚Äù. In this challenge, participants had to classify images into 1000 classes, like ‚ÄúZebra‚Äù, ‚ÄúDalmatian‚Äù, and ‚ÄúDishwasher‚Äù.\n",
        "\n",
        "Despite the objects it was trained to classify are quite different compared to skin diseases images, the features detection part of such pretrained model are often reused to classify completely different images.  \n",
        "\n",
        "I initially selected __[DenseNet-161](https://arxiv.org/pdf/1608.06993.pdf)__ as a good model to reuse because it has (had) __improved performance over others on ImageNet__ and it introduces an interesting architecture with __each layer taking all preceding feature-maps as input__. Moreover despite it has a lot of layers, the time to train a single epoch is very similar to simpler pretrained models.\n",
        "\n",
        "Picture below shows the smaller DenseNet-121.\n",
        "\n",
        "![title](https://cdn-images-1.medium.com/max/1000/1*BJM5Ht9D5HcP5CFpu8bn7g.png?raw=1)\n",
        "\n",
        "We just need two modifications to the pretrained model (see configure_model function below):\n",
        " - üìùas skin diseases images are still different compared to cats, dogs, cars... I __freeze the parameters of the two first blocks (feature extraction), and I train the parameters of the two last blocks (fine-tuning)__ (Dense Block 3 and Dense Block 4). __This step is crucial!__ Otherwise the model caps at 80% accuracy in train set, and swings between 50-60% in test set. üìù\n",
        " - and then __replace the last fully connected layer (the classifier itself) so that it outputs 3 classes__ instead of 1000.\n",
        "\n",
        "I tried two different final classifiers and retained the first one:\n",
        "- one layer with a single Linear classifier;\n",
        "- two layers, with batch normalization to prevent vanishing gradients and dropout for regularization.  \n",
        "\n",
        "Both outputs raw scores that will later be transformed as probabilities using softmax."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-02-14T07:53:36.157501Z",
          "start_time": "2019-02-14T07:53:36.153511Z"
        },
        "id": "iMsLSD-E_V9P",
        "outputId": "e8261027-002a-48d2-9098-dc777ce8a72a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "import torchvision.models as models\n",
        "import torch.nn as nn\n",
        "\n",
        "def set_parameter_requires_grad(parameters, feature_extract):\n",
        "  # set parameters that should be trained\n",
        "  \n",
        "  for param in parameters:\n",
        "      param.requires_grad = not feature_extract\n",
        "\n",
        "def configure_model(model, final_classifier, num_classes, feature_extract, skip_requires_grad = False):\n",
        "  \n",
        "  if not skip_requires_grad:\n",
        "    set_parameter_requires_grad(model.parameters(), feature_extract)\n",
        "    \n",
        "  return nn.Linear(final_classifier.in_features, num_classes)"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "time: 5.26 ms\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZvIBDPnXP8TP"
      },
      "source": [
        "The initialize_model function below allows you to create and configure a few well-known models:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-02-14T07:53:37.164290Z",
          "start_time": "2019-02-14T07:53:37.153302Z"
        },
        "id": "-K09CkCw_V9S",
        "outputId": "4ec21914-da0d-48f0-b33c-1eb337cdbc58",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "import pretrainedmodels\n",
        "\n",
        "def initialize_model(model_name, num_classes, use_pretrained=False, feature_extract=True):\n",
        "  \n",
        "    model = None\n",
        "    model_name = model_name.lower()\n",
        "\n",
        "    if model_name == \"resnet\":\n",
        "        \"\"\" Resnet152\n",
        "        \"\"\"\n",
        "        model = models.resnet152(pretrained=use_pretrained)\n",
        "        model.fc = configure_model(model, model.fc, num_classes, feature_extract)\n",
        "\n",
        "    elif model_name == \"alexnet\":\n",
        "        \"\"\" Alexnet\n",
        "        \"\"\"\n",
        "        model = models.alexnet(pretrained=use_pretrained)\n",
        "        model.classifier[6] = configure_model(model, model.classifier[6], num_classes, feature_extract)\n",
        "\n",
        "    elif model_name == \"vgg\":\n",
        "        \"\"\" VGG11_bn\n",
        "        \"\"\"\n",
        "        model = models.vgg11_bn(pretrained=use_pretrained)\n",
        "        model.classifier[6] = configure_model(model, model.classifier[6], num_classes, feature_extract)\n",
        "\n",
        "    elif model_name == \"squeezenet\":\n",
        "        \"\"\" Squeezenet\n",
        "        \"\"\"\n",
        "        model = models.squeezenet1_0(pretrained=use_pretrained)\n",
        "        set_parameter_requires_grad(model.parameters(), feature_extract)\n",
        "        model.classifier[1] = nn.Conv2d(512, num_classes, kernel_size=(1,1), stride=(1,1))\n",
        "        model.num_classes = num_classes\n",
        "\n",
        "    elif model_name == \"densenet\":\n",
        "        \"\"\" Densenet\n",
        "        \"\"\"\n",
        "        model = models.densenet161(pretrained=use_pretrained)\n",
        "        model.classifier = configure_model(model, model.classifier, num_classes, feature_extract)\n",
        "\n",
        "    elif model_name == \"inception3\":\n",
        "        \"\"\" Inception v3\n",
        "        \"\"\"\n",
        "        model = models.inception_v3(pretrained=use_pretrained)\n",
        "        # Handle the auxilary net\n",
        "        model.AuxLogits.fc = configure_model(model, model.AuxLogits.fc, num_classes, feature_extract)\n",
        "        # Handle the primary net\n",
        "        model.fc = configure_model(model, model.fc, num_classes, feature_extract, skip_requires_grad=True)\n",
        "        \n",
        "    elif model_name in pretrainedmodels.__dict__['model_names']:\n",
        "        \"\"\" pretrainedmodels\n",
        "        \"\"\"        \n",
        "        model = pretrainedmodels.__dict__[model_name](num_classes=1000, pretrained=('imagenet' if use_pretrained else None))\n",
        "        model.last_linear = configure_model(model, model.last_linear, num_classes, feature_extract)\n",
        "\n",
        "    else:\n",
        "        print(f\"Invalid model name: {model_name}, exiting...\")\n",
        "        exit()\n",
        "\n",
        "    # Send the model to GPU if any\n",
        "    return model.to(device), get_image_size(model_name)"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "time: 1.27 s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gW9bFl5nQpQa"
      },
      "source": [
        "## Feature extraction or fine-tuning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qeYWLX67N0ra"
      },
      "source": [
        "Choosing the parameters for feature extraction or fine-tuning (first_trained_module in code below) is __almost random guess__...  \n",
        "\n",
        "üìùSo I introduced(*) __fine_tuning_module_rounds__: while training, it automatically adds one more module for fine-tuning if ROC AUC does not increase within given n epochs rounds. It is set to Infinite by default for the behavior to be consciously chosen.üìù  \n",
        "\n",
        "<i>(*) Most of my models where trained with a bit of \"guess\". Only the last two trials had fine_tuning_module_rounds enabled.</i>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2KXohntfONqP",
        "outputId": "a565addf-cf46-434d-9950-aff4a5ead6d1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "def find_first_trained_module(model, start_module_index=np.Inf, start_module_name=None):\n",
        "  \n",
        "  for i, (name, module) in enumerate(reversed(list(model.named_modules()))):\n",
        "    set_parameter_requires_grad(module.parameters(), feature_extract=False)\n",
        "    if (start_module_name == name or i > start_module_index) and len(list(module.parameters())) > 0:       \n",
        "      return i, name, get_optimizer(model, name if start_module_name is None else start_module_name)\n",
        "\n",
        "def set_trained_modules(model, optimizer, epoch, best_epoch, since_epoch, first_trained_module, first_trained_module_i, best_auc, test_phase, fine_tuning_module_rounds=np.Inf):\n",
        "  \n",
        "  if (epoch == fine_tuning_module_rounds + best_epoch + 1) or (best_epoch < since_epoch and epoch == fine_tuning_module_rounds + since_epoch + 1) :\n",
        "    since_epoch = epoch\n",
        "    first_trained_module_i, first_trained_module, optimizer = find_first_trained_module(model, first_trained_module_i + 1)\n",
        "    \n",
        "  logging.info(get_params_to_train_str(model, first_trained_module_i, f' (since epoch {since_epoch:2.0f})', \n",
        "                                       fine_tuning_rounds_str = '' if (fine_tuning_module_rounds==np.Inf or epoch==1) else f' ‚Üë Fine-tuning goes up one module if {test_phase} ROC AUC < {best_auc:.3f} at end of epoch {(best_epoch+fine_tuning_module_rounds) if best_epoch<since_epoch else (since_epoch+fine_tuning_module_rounds):2.0f} ‚Üë '))\n",
        "\n",
        "  return since_epoch, first_trained_module_i, first_trained_module, optimizer"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "time: 13.8 ms\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3cGVR_rjOUHc",
        "outputId": "ed576a12-3174-418d-ac92-b27903a62041",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "def trail_str(str, max_length, last_chars=''):\n",
        "  \n",
        "  if len(str) > max_length-2-len(last_chars):\n",
        "    return str[0:max_length-3] + '...' + last_chars\n",
        "  return str.ljust(max_length-len(last_chars)) + last_chars\n",
        "    \n",
        "def get_params_to_train_str(model, first_trained_module_i, since_epoch_str, fine_tuning_rounds_str=None):\n",
        "  \n",
        "  modules = [(('= ' if i > first_trained_module_i else '~ ') + f'{name}') for (i, (name, module)) in enumerate(reversed(list(model.named_modules()))) if name]                                                                                                             \n",
        "  \n",
        "  params_to_train, n_params = get_params_to_train(model)  \n",
        "  total_parameters = len(list(model.parameters()))\n",
        "  \n",
        "  feature_extract_modules = modules[first_trained_module_i+1:first_trained_module_i+3] + ['= ...'] + modules[::-1][0:3][::-1]\n",
        "  feature_extract_modules[0] = trail_str(feature_extract_modules[0], 40)                + f' ‚Üë Fixed feature extractor on {str(total_parameters-n_params).rjust(4)} parameters out of {str(total_parameters).rjust(4)} in modules above   ‚Üë'\n",
        "  fine_tuning_modules = modules[0:3] + ['~ ...'] + modules[first_trained_module_i-1:first_trained_module_i+1]\n",
        "  fine_tuning_modules[-1] = trail_str(fine_tuning_modules[-1], 40, since_epoch_str) + f' ‚Üì Fine-tuning                {str(n_params).rjust(4)} parameters out of {str(total_parameters).rjust(4)} in modules below   ‚Üì'\n",
        "  \n",
        "  return '\\n'.join(reversed(feature_extract_modules)) \\\n",
        "      + f'\\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~{fine_tuning_rounds_str}~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\\n' \\\n",
        "       + '\\n'.join(reversed(fine_tuning_modules))\n",
        "\n",
        "def get_params_to_train(model, max_params=6):\n",
        "  \n",
        "  params_to_train = list(get_params_to_optimize(model).keys())\n",
        "  n_params = len(params_to_train)\n",
        "  \n",
        "  if n_params > max_params:\n",
        "    params_to_train = params_to_train[0:max_params//2] + ['...'] + params_to_train[::-1][0:max_params//2][::-1]\n",
        "  \n",
        "  return params_to_train, n_params"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "time: 21.6 ms\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gFB-7ihX7ugq"
      },
      "source": [
        "## Loss function"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XCsqCJnW_0pQ"
      },
      "source": [
        "__CrossEntropyLoss__ is the appropriate loss function for a classification model that outputs raw scores for each class.  \n",
        "\n",
        "__Melanoma is the deadliest form of cancer__. Despite my samples will correctly be balanced at each iteration, I want to __penalize by 50% the loss on melanoma__."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-02-14T07:53:48.328275Z",
          "start_time": "2019-02-14T07:53:48.310323Z"
        },
        "id": "yPB2dOQf_V9Y",
        "outputId": "faac2a89-c791-43d6-aa21-c03b9dfce0bb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "#rescaling weight given to each class\n",
        "weight = [1.5, 1, 1]\n",
        "criterion = nn.CrossEntropyLoss(torch.Tensor(weight).to(device))"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "time: 3.97 s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "scUkkZpE73CH"
      },
      "source": [
        "## Optimizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d1ECOsemO000",
        "outputId": "ca0cc1eb-31ab-4354-8ea9-97ecb75c243c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "def get_params_to_optimize(model):\n",
        "  # Gather the parameters to be optimized/updated in this run. \n",
        "  params_to_update = {}\n",
        "  for name,param in model.named_parameters():\n",
        "      if param.requires_grad == True:\n",
        "          params_to_update[name] = param\n",
        "  \n",
        "  return params_to_update"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "time: 3.12 ms\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hnJke-ByQxYU"
      },
      "source": [
        "Whatever the optimizer, he needs to know which parameters of the model he should optimize. If we are finetuning we will be updating all parameters. However, if we are doing feature extract method, we will only update the parameters that we have just initialized, i.e. the parameters with requires_grad is True."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OriaegfT9Is0"
      },
      "source": [
        "I selected __Adam optimizer__ because it achieves good results fast. This algorithm is an extension to stochastic gradient descent that has recently seen broader adoption for deep learning applications in computer vision. It combines the advantages of two other extensions of stochastic gradient descent : AdaGrad and RMSProp."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Qm7BNUg81nv",
        "outputId": "a47b143d-af2e-4c5b-fa92-a60e1376f511",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "def get_optimizer(model, first_fine_tuned_module):\n",
        "\n",
        "  return optim.Adam(get_optimizer_params_group(model, first_fine_tuned_module), lr=0.001, weight_decay=1e-6)"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "time: 1.78 ms\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "omfIcyEzJIBR"
      },
      "source": [
        "üìù It‚Äôs common to use a smaller learning rate for ConvNet weights that are being fine-tuned, in comparison to the (randomly-initialized) weights for the new linear classifier that computes the class scores of your new dataset. This is because we expect that the ConvNet weights are relatively good, so we don‚Äôt wish to distort them too quickly and too much (especially while the new Linear Classifier above them is being trained from random initialization).üìù  \n",
        "\n",
        "Although the usual practice is to reduce the initial learning rate by 10 times compared to that used for scratch training, I preferred to use different learning rates according to the universality / specificity of what is captured by each layer. Indeed, the first layers capture universal features while the latter capture dataset-specific ones."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mV2LTvmHtkOM",
        "outputId": "c0d0e0ef-501f-4e35-d2c1-ecb7b9508002",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "def get_optimizer_params_group(model, first_fine_tuned_module, classifier_lr=0.001, highest_layer_lr=2*1e-4, lowest_layer_lr=0.5e-4, weight_decay=1e-6):\n",
        "  \n",
        "  modules = list(get_top_modules(model, first_fine_tuned_module))\n",
        "  modules = [m for m in modules if not any((m['name']+'.') in m2['name'] for m2 in modules)]\n",
        "  learning_rates = list(np.linspace(lowest_layer_lr, highest_layer_lr, len(modules)-1)) + [classifier_lr]\n",
        "  \n",
        "  for m, lr in zip(modules, learning_rates):\n",
        "    yield {'group_name': m['name'], 'params': m['module'].parameters(), 'lr': lr, 'weight_decay': weight_decay}"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "time: 6.93 ms\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wDNIpoRQT_BR",
        "outputId": "e18a0e2f-aa78-4745-873e-292527e07550",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "from collections import OrderedDict\n",
        "  \n",
        "def get_top_modules(model, first_fine_tuned_module):\n",
        "            \n",
        "  module_names = [f'{name}' for (i8, (name, module)) in enumerate(list(model.named_modules())) if (name and len(list(module.parameters()))>0)]\n",
        "  top_modules = []\n",
        "  \n",
        "  depth=0\n",
        "  while len(top_modules)<10 and depth<4:\n",
        "    depth+=1\n",
        "    top_modules = list(OrderedDict.fromkeys([get_top_module_name(name, depth) for name in module_names]))\n",
        "\n",
        "  first_fine_tuned_top_module = sorted([m for m in top_modules if m in first_fine_tuned_module], key=len)[-1]\n",
        "\n",
        "  fine_tuned_modules = top_modules[top_modules.index(first_fine_tuned_top_module):]\n",
        "  for name in fine_tuned_modules:\n",
        "    yield {'name': name, 'module': get_sub_module(model, name)}\n",
        "\n",
        "def get_top_module_name(name, depth=1):\n",
        "  \n",
        "  if depth==0: \n",
        "    return name\n",
        "  else:\n",
        "    return '.'.join(name.split('.')[0:depth]) if name.count('.')>=(depth-1) else get_top_module_name(name, depth-1)\n",
        "  \n",
        "def get_sub_module(module, name):\n",
        "  \n",
        "  name = name.split('.')\n",
        "  module = module[int(name[0])] if name[0].isdigit() else getattr(module, name[0])\n",
        "  \n",
        "  return module if len(name)==1 else get_sub_module(module, '.'.join(name[1:]))"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "time: 23.5 ms\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ObFLVasVXrsx"
      },
      "source": [
        "## Choosing the right metric \n",
        "What is the single metric to take into account to determine if the model is good?\n",
        "\n",
        "For classification problems, the following metrics are often usefull:\n",
        "- Precision-Recall\n",
        "- ROC-AUC\n",
        "- Accuracy\n",
        "- Log-loss\n",
        "\n",
        "In this challenge, the decision is easy because the performance is measured against the __mean ROC-AUC of melanoma and seborrheic keratosis__."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MNynqFxe8K54",
        "outputId": "87bc0175-5339-4de0-9633-0fae2d253700",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "from sklearn.metrics import roc_curve, auc\n",
        "\n",
        "def get_roc_auc(y_true, y_pred, plot=False):\n",
        "    \"\"\"\n",
        "    return ROC AUC Score for melanoma, seborrheic_keratosis, and their mean\n",
        "    \"\"\"\n",
        "\n",
        "    # initialize dictionaries and array\n",
        "    fpr = dict()\n",
        "    tpr = dict()\n",
        "    roc_auc = np.zeros(3)\n",
        "    \n",
        "    # prepare for figure\n",
        "    if plot:\n",
        "      plt.figure()\n",
        "      colors = ['aqua', 'cornflowerblue']\n",
        "\n",
        "    # for both classification tasks (categories 1 and 2)\n",
        "    for i in range(2):\n",
        "        # obtain ROC curve\n",
        "        fpr[i], tpr[i], _ = roc_curve(y_true[:,i], y_pred[:,i])\n",
        "        # obtain ROC AUC\n",
        "        roc_auc[i] = auc(fpr[i], tpr[i])\n",
        "        if plot:\n",
        "          # plot ROC curve\n",
        "          plt.plot(fpr[i], tpr[i], color=colors[i], lw=2,\n",
        "                   label='ROC curve for task {d} - class {c} (area = {f:.3f})'.format(d=i+1, c=('melanoma' if i==0 else 'seborrheic_keratosis'), f=roc_auc[i]))\n",
        "          \n",
        "    # get score for category 3\n",
        "    roc_auc[2] = np.average(roc_auc[:2])\n",
        "    \n",
        "    if plot:\n",
        "      # format figure\n",
        "      plt.plot([0, 1], [0, 1], 'k--', lw=2)\n",
        "      plt.xlim([0.0, 1.0])\n",
        "      plt.ylim([0.0, 1.05])\n",
        "      plt.xlabel('False Positive Rate')\n",
        "      plt.ylabel('True Positive Rate')\n",
        "      plt.title(f'ROC curves - (mean area = {roc_auc[2]:.3f})')\n",
        "      plt.legend(loc=\"lower right\")\n",
        "      ax = plt.gca()\n",
        "      ax.set_facecolor('white')\n",
        "      plt.show()\n",
        "\n",
        "      # print scores\n",
        "      for i in range(3):\n",
        "          print('Category {d} Score: {f:.3f}'. format(d=i+1, f=roc_auc[i]))\n",
        "        \n",
        "    return roc_auc"
      ],
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "time: 35.6 ms\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dsFbGvNhSFKG"
      },
      "source": [
        "Have we achieved a higher ROC AUC metric in the validation set? Let's save it!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "THFkuJMA75L-",
        "outputId": "9ee4969a-5500-4df7-ef5f-a7ddd6b44841",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "import copy\n",
        "\n",
        "def save_model(save_path, model, best_epoch_auc):\n",
        "  # we achieved a higher ROC AUC metric in validation set!\n",
        "  # let's save it\n",
        "  \n",
        "  if save_path is not None:\n",
        "    logging.info('=> Saving model')\n",
        "    torch.save(model.state_dict(), save_path.replace('.pt', f'_{best_epoch_auc:.4f}.pt'))\n",
        "  \n",
        "  best_model_wts = copy.deepcopy(model.state_dict())                    \n",
        "  \n",
        "  return best_model_wts, best_epoch_auc"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "time: 4.67 ms\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1AEiv70OuhYp"
      },
      "source": [
        "... and take the opportunity to write the code to reload this saved model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k8YiGzGduP3U",
        "outputId": "881df22e-d2e7-4dc2-d20b-2f8953690f97",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "def load_model(state_dict_fn):\n",
        "  \n",
        "  if os.path.exists(os.path.join(os.getcwd(), state_dict_fn)):\n",
        "    # already downloaded\n",
        "    state_dict_fn = os.path.join(os.getcwd(), state_dict_fn)\n",
        "  elif os.path.exists(os.path.join(google_drive_shared_path, state_dict_fn)):\n",
        "    # available on my own google drive\n",
        "    state_dict_fn = os.path.join(google_drive_shared_path, state_dict_fn)\n",
        "  else:\n",
        "    state_dict_fn = os.path.join(os.getcwd(), state_dict_fn)\n",
        "    download_file(state_dict_fn, quiet=True)\n",
        "  \n",
        "  model, image_size = initialize_model(get_model_name(state_dict_fn).lower(), len(classes))\n",
        "  model.load_state_dict(torch.load(state_dict_fn))\n",
        "  \n",
        "  return model"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "time: 8.11 ms\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SwSbo3AtuZmN",
        "outputId": "4cd909b8-b9a0-4c1d-c446-7b74008e9bb6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "def get_model_name(fn):\n",
        "  \n",
        "  model_name = fn.split(os.path.sep)[-1].split('_')[0].split('.')[0]\n",
        "  \n",
        "  return f'{model_name}154' if model_name.lower()=='senet' else model_name"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "time: 2.03 ms\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zzML9eKwuwHF"
      },
      "source": [
        "... and finally release the model when we don't need it anymore, so that it frees GPU memory."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vt02LAi2uR6R",
        "outputId": "cbc071b9-9b51-4d82-f2ff-6c8ae1ea5205",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "def release_model(model):\n",
        "  # free GPU memory\n",
        "  \n",
        "  if model is not None:\n",
        "    del model\n",
        "    torch.cuda.empty_cache()"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "time: 1.77 ms\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZBuzJLY09_2q"
      },
      "source": [
        "# Train the Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XNYXbnmdw5Q4"
      },
      "source": [
        "## Helper functions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DBTOiIayS8JH"
      },
      "source": [
        "### Logging\n",
        "I want to log some information before, after and while training. Here is what I needed:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i5acFT6TqVBD",
        "outputId": "ea5e34ae-38c9-45e6-ab19-dca4865e4dbe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "def get_model_path(model_name, extension):\n",
        "  # return a unique name for logging and saving model\n",
        "  \n",
        "  i = 1\n",
        "  path = google_drive_shared_path if os.path.exists(google_drive_shared_path) else os.getcwd()\n",
        "  while len(glob.glob(os.path.join(path, f'{model_name}_{i}*.{extension}'))):\n",
        "    i+=1\n",
        "  return os.path.join(path, f'{model_name}_{i}.{extension}')"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "time: 3.45 ms\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pGiCKi2kNtNH",
        "outputId": "fa66ec74-3123-4b7d-bde8-533ccc74c03a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "import logging\n",
        "\n",
        "def init_log(model_name):\n",
        "  \n",
        "  # reset handlers\n",
        "  rootLogger = logging.getLogger()\n",
        "  rootLogger.handlers = []\n",
        "  rootLogger.setLevel(logging.INFO)\n",
        "\n",
        "  # log to file\n",
        "  save_path = get_model_path(model_name, 'pt')\n",
        "  fileHandler = logging.FileHandler(save_path.replace('.pt', '.log'))\n",
        "  fileHandler.terminator = '\\r\\n'\n",
        "  rootLogger.addHandler(fileHandler)\n",
        "\n",
        "  # log to console\n",
        "  consoleHandler = logging.StreamHandler()\n",
        "  rootLogger.addHandler(consoleHandler)\n",
        "  \n",
        "  return save_path"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "time: 9.24 ms\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "daOb2ftMdeuU",
        "outputId": "7d3b247d-c239-4d2c-a332-1307727d6927",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "from collections import Counter\n",
        "  \n",
        "def log_model_summary(model, criterion, optimizer, phase_data_dirs, over_sampling, cv, start=True, time_elapsed=0):\n",
        "  \n",
        "  model_name = model.__class__.__name__\n",
        "  first_word = 'Start' if start else 'End'\n",
        "  logging.info(f'{first_word} training {model_name} model')\n",
        "  \n",
        "  if not start:\n",
        "    logging.info('Training complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\n",
        "  \n",
        "  y = get_true_labels(phase_data_dirs['train'])\n",
        "  counter = Counter(y)\n",
        "  logging.info(f'  Class distribution ({np.sum(list(counter.values()))}): ' + ', '.join(f'{count} ' + classes[int(disease)] for disease, count in counter.items()))\n",
        "  logging.info(f'  Oversampling: {over_sampling}')\n",
        "  logging.info(f'  Cross validation: {cv}')\n",
        "  \n",
        "  logging.info(f'  Loss function: {criterion} - Weights: {criterion.weight}')\n",
        "  logging.info(f'  Optimizer: {optimizer}')\n",
        "  \n",
        "  train_transforms = get_transform(model_name, 'train')\n",
        "  logging.info(f'\\n  Train tranforms: {train_transforms}')\n",
        "  logging.info('')"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "time: 20.8 ms\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PVfeAXIhT9Mc"
      },
      "source": [
        "### Live plotting\n",
        "_\"Don't train deep learning models blindfolded! Be impatient and look at each epoch of your training!\"_  \n",
        "\n",
        "I forked [LiveLossPlot](https://github.com/sebastienlange/livelossplot) to enhance it and [add support for custom series](https://github.com/stared/livelossplot/pull/46) and [add support for marker on higher/lower scores](https://github.com/stared/livelossplot/pull/48).\n",
        "\n",
        "While ROC AUC is the choosen metric, I also plot after each epoch log-loss and accuracy."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1DEW7geB72Ky",
        "outputId": "958b8f7d-8baa-456c-9e25-fe259e230fa8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "def update_liveplot(logs, phase, epoch_loss, epoch_acc, epoch_auc):\n",
        "  ## update data for LiveLossPlot\n",
        "    \n",
        "  logs[f'{phase}_log loss'] = epoch_loss\n",
        "  logs[f'{phase}_accuracy'] = epoch_acc\n",
        "  logs[f'{phase}_ROC AUC'] = epoch_auc[2]\n",
        "  \n",
        "  return logs"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "time: 3.36 ms\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HVvzJkbnU5HV"
      },
      "source": [
        "I also display on the progress bar log-loss, accuracy and even per-class accuracy after each batch... That's very addictive üòÉ"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bZLFCVRM8D8z",
        "outputId": "0588d758-20c5-49dc-d401-6b4d3bc05e02",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "def get_running_status(phase, running_loss, running_accuracy):\n",
        "  # update progress bar with current loss and accuracy\n",
        "  # and even pre-class accuracy\n",
        "  \n",
        "  progresses = []\n",
        "  progresses.append(f'{phase}_loss={running_loss/sum(np.array(list(running_accuracy.values()))[:,1]):.3f}')\n",
        "  progresses.append(f'{phase}_acc={compute_accuracy(running_accuracy):.3f}')\n",
        "  \n",
        "  for disease in classes:\n",
        "    progresses.append(f'{disease[0:5]}_acc={running_accuracy[disease][0]/running_accuracy[disease][1]:.3f}')\n",
        "\n",
        "  return ', '.join(progresses)"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "time: 6.69 ms\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TNugMfgHWQfB"
      },
      "source": [
        "### Computing stats\n",
        "The final classifier output raw scores.  \n",
        "\n",
        "To compute ROC AUC score, I need probabilities and one-hot encoded true labels:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bDKjUhVN8GYi",
        "outputId": "6354439f-3e1b-4540-a040-f243707f5e60",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "def compute_batch_metrics(inputs, labels, outputs, loss, running_loss, one_hot_labels, outputs_probabilities, running_accuracy):\n",
        "  # for a given batch it computes the one_hot_labels (one hot encoding)\n",
        "  # and the probabilities for each class\n",
        "  \n",
        "  # use LabelBinarizer instead ? https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.LabelBinarizer.html#sklearn.preprocessing.LabelBinarizer\n",
        "  one_hot_labels = np.append(one_hot_labels, one_hot_encoding(labels.data.cpu().numpy()), axis=0)\n",
        "\n",
        "  # track running prediction probabilities\n",
        "  batch_probabilities = torch.nn.Softmax(dim=1)(outputs.data).cpu().numpy()\n",
        "  outputs_probabilities = np.append(outputs_probabilities, batch_probabilities, axis=0)\n",
        "  \n",
        "  # track running loss\n",
        "  running_loss += loss.item() * inputs.size(0)\n",
        "  \n",
        "  # track running accuracy\n",
        "  _, predicted_labels = torch.max(outputs, 1)  \n",
        "  for i, disease in enumerate(classes):\n",
        "    # nb corrects for disease\n",
        "    running_accuracy[disease][0] += torch.sum(predicted_labels[labels.data==i]==labels.data[labels.data==i]).cpu().numpy()\n",
        "    # nb processed for disease\n",
        "    running_accuracy[disease][1] += labels.data[labels.data==i].size(0)\n",
        "  \n",
        "  return running_loss, one_hot_labels, outputs_probabilities, running_accuracy"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "time: 13.8 ms\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PdvK7a6DgZ1-",
        "outputId": "6faff174-bf18-46fc-ed61-875a7d007049",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "def one_hot_encoding(data):\n",
        "  \n",
        "  one_hot = np.zeros((data.size, 3))\n",
        "  one_hot[np.arange(data.size),data] = 1\n",
        "  \n",
        "  return one_hot"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "time: 3.73 ms\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MJ6YBMVm-yV_"
      },
      "source": [
        "## Cross-validation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8dbjAc79IoIl"
      },
      "source": [
        "Finally I use the __[RepeatedStratifiedKFold](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.RepeatedStratifiedKFold.html) cross validator__ with 5 splits for the following reasons:\n",
        "- I do not want to leave aside my precious 853 validation samples... they will also be used in training;\n",
        "- the folds are made by preserving the percentage of samples for each class;\n",
        "- every 5 epochs, the folds are resampled."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j70jxP6nDMgX",
        "outputId": "1915b0ae-48fa-46b3-e622-f6fcac50f701",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# merge train, valid and more folders as cross-validation will be used\n",
        "more = ['more'] if download_more_images else []\n",
        "phase_data_dirs = {'train': [data_dir[d] for d in sorted(['train', 'valid'] + more)], \n",
        "                   'valid': [data_dir[d] for d in sorted(['train', 'valid'] + more)],\n",
        "                  }\n",
        "\n",
        "phase_data_dirs_resized = {'train': [data_dir[d] + '_resized' for d in sorted(['train', 'valid'] + more)], \n",
        "                           'valid': [data_dir[d] + '_resized' for d in sorted(['train', 'valid'] + more)],\n",
        "                          }"
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "time: 6.92 ms\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GLGQze-GIJBn",
        "outputId": "6b848428-e601-4d90-d027-c4a612dfdba8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "def get_true_labels(data_dirs):\n",
        "  # it returns class_id for each sample in the provided folders [0 0 0 1 1 2]\n",
        "  \n",
        "  y = []\n",
        "  for d in data_dirs:\n",
        "    for class_id, disease in enumerate(classes):\n",
        "      y = np.append(y, np.full(len(glob.glob(os.path.join(d, disease, '*.jpg'))), class_id))\n",
        "  return y"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "time: 4.73 ms\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uJy4TaEe-5ar",
        "outputId": "ac8eb05b-5ef5-44dc-a83c-e2be6f456ade",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "from sklearn.model_selection import RepeatedStratifiedKFold, StratifiedKFold\n",
        "\n",
        "def get_kfold_splitter(phase_y, num_epochs, cv=5, test_split=10):\n",
        "  # return a lambda to create new k-folds every k iterations\n",
        "  # the lambda will they return the training_indices and validation_indices\n",
        "    \n",
        "  X = list(range(len(phase_y['train'])))\n",
        "  y = phase_y['train']\n",
        "  \n",
        "  if test_split is not None:\n",
        "    # return one single split to have train_and_validation indices, and test split (will never be trained)\n",
        "    train_valid_indices, test_indices = next(iter(StratifiedKFold(n_splits=test_split).split(X, y)))\n",
        "    y = [phase_y['train'][i] for i in train_valid_indices]\n",
        "  else:\n",
        "    train_valid_indices = X\n",
        "    test_indices = list(range(len(phase_y['test']))) if 'test' in phase_y else []\n",
        "  \n",
        "  rskf = RepeatedStratifiedKFold(n_splits=cv, n_repeats=int(num_epochs/5))\n",
        "  \n",
        "  return lambda: [get_phases_indices(train_indices, valid_indices, test_indices) for (train_indices, valid_indices) in rskf.split(train_valid_indices, y)]"
      ],
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "time: 32.5 ms\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tcOD2qYsyJ0M",
        "outputId": "5c461d3f-512e-44c1-f33e-92d4eab4eab0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "def get_split_strategy(phase_y, num_epochs, cv=5, test_split=10):\n",
        "  \n",
        "  if cv is None:\n",
        "    return lambda: [{phase: list(range(len(y))) for (phase, y) in phase_y.items()} for idx in range(num_epochs)]\n",
        "  \n",
        "  return get_kfold_splitter(phase_y, num_epochs, cv, test_split) "
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "time: 4.2 ms\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oidXbnpBe2ao",
        "outputId": "27c66dd1-ce24-45c7-8711-d27c75d505ee",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "def get_phases_indices(train_indices, valid_indices, test_indices):\n",
        "  \n",
        "  phases_indices = {'train': train_indices, 'valid': valid_indices}\n",
        "  \n",
        "  if len(test_indices) > 0:\n",
        "    phases_indices['test'] = test_indices\n",
        "  \n",
        "  return phases_indices"
      ],
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "time: 3.49 ms\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ksaK0GqvXlNh"
      },
      "source": [
        "### Concatening Image Folders\n",
        "PyTorch [ImageFolder](https://pytorch.org/docs/stable/torchvision/datasets.html#imagefolder) only supports one single root path. However, my images for training are stored in multiple folders: train, more, and even valid as I'm using cross-validation.  \n",
        "\n",
        "So I need [ConcatDataset](https://pytorch.org/docs/stable/data.html#torch.utils.data.ConcatDataset) to concatenate these folders."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aRrD0cWfE_Yd",
        "outputId": "b984346b-6dbb-4c6d-eaa8-1bd118a747bf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "from torch.utils.data import ConcatDataset\n",
        "\n",
        "def get_dataset(transform, dirs):\n",
        "  # it returns as many ImageFolder as we have paths in a given phase \n",
        "  # e.g. If cross-validation is used and I have downloaded more data, my training set could have images from 'train', 'valid', and 'more' folders\n",
        "\n",
        "  dataset = [datasets.ImageFolder(d, transform=transform) for d in dirs]\n",
        "  \n",
        "  return dataset[0] if len(dataset)==1 else ConcatDataset(dataset)"
      ],
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "time: 9.61 ms\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eqfBrH7qRWLE"
      },
      "source": [
        "The cross validator splits indices. So I also need to load the images with a sampler that accepts indices: [SubsetRandomSampler](https://pytorch.org/docs/stable/data.html#torch.utils.data.SubsetRandomSampler) is what I need.  \n",
        "\n",
        "The indices will also be shuffled as it is required for the train phase. The same goes for the other phases, although this is neither necessary nor problematic."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YYLTAaCJM5rj",
        "outputId": "9a4f34f3-e341-4c83-9440-badfa941ef0e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "from torch.utils.data import SubsetRandomSampler\n",
        "\n",
        "def get_data_loader(phase, model, folders, indices=None, phase_y=None, over_sampling=False, best_auc=1, n_crops=1, resize=0.75):\n",
        "\n",
        "  dataset = get_dataset(get_transform(model.__class__.__name__, phase, resize, n_crops=n_crops), folders)\n",
        "  \n",
        "  sampler = None\n",
        "  \n",
        "  if indices is not None:\n",
        "    sampler = SubsetRandomSampler(balance_indices(indices, phase_y['train'], 1) if (over_sampling and phase=='train') else indices)        \n",
        "  \n",
        "  return torch.utils.data.DataLoader(dataset, batch_size=batch_size[phase], sampler=sampler, num_workers=num_workers)"
      ],
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "time: 7.86 ms\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yxau3jnrGGv9"
      },
      "source": [
        "## Train functions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G6mVSzB8cO7w"
      },
      "source": [
        "__Here we are finally!__  üòÖ  \n",
        "\n",
        "The function below will train the model for a given number of epochs.\n",
        "\n",
        "After each epoch, it evaluates the model against the validation set, and its parameters are saved if validation ROC AUC has increased. It's a good way to do early stopping when the model starts to overfit to the training set. \n",
        "\n",
        "While training, it plots live training and validation loss, accuracy and ROC AUC."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-02-14T07:53:30.595921Z",
          "start_time": "2019-02-14T07:53:30.582955Z"
        },
        "id": "IZM5i7wb_V8-",
        "outputId": "1b5cf6a2-572b-47d7-fb4a-6a9bdae1f9cd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "import time\n",
        "\n",
        "from livelossplot import PlotLosses\n",
        "\n",
        "def train_model(model, criterion, phase_data_dirs, over_sampling=True, num_epochs=25, cv=5, test_split=None, first_trained_module=None, best_auc = 0.0, fine_tuning_module_rounds=np.Inf, n_crops_stages={5: 0.875, 10: 0.890}):\n",
        "\n",
        "  model, optimizer, first_trained_module_i, save_path, starting_time, best_model_wts, phase_y, split_strategy, live_loss_plot \\\n",
        "    = init_training(model, criterion, phase_data_dirs, num_epochs, over_sampling, cv, test_split, first_trained_module) \n",
        "  \n",
        "  best_epoch = 1\n",
        "  since_epoch = 1\n",
        "\n",
        "  try:        \n",
        "    for epoch, phases_indices in enumerate(split_strategy(), 1):\n",
        "\n",
        "      logs = {}\n",
        "      \n",
        "      current_best_auc = best_auc\n",
        "      since_epoch, first_trained_module_i, first_trained_module, optimizer = set_trained_modules(model, optimizer, epoch, best_epoch, since_epoch, first_trained_module, first_trained_module_i, best_auc, list(phases_indices.keys())[-1], fine_tuning_module_rounds)\n",
        "\n",
        "      # Each epoch has a training and validation phase\n",
        "      for phase, indices in phases_indices.items():\n",
        "       \n",
        "        data_loader = init_epoch(model, phase, phase_data_dirs[phase], indices, phase_y, over_sampling, epoch, num_epochs, cv, phase==list(phases_indices.keys())[-1], n_crops_stages=n_crops_stages, best_auc=best_auc)\n",
        "\n",
        "        # run a single epoch\n",
        "        logs, best_auc, best_model_wts, _, _ = run_epoch(phase, data_loader, model, criterion, optimizer, logs, best_auc, best_model_wts, save_path.replace('.pt', f'_{first_trained_module}.pt'), save_phase = list(phases_indices.keys())[-1])\n",
        "        \n",
        "      if best_auc > current_best_auc:\n",
        "        best_epoch = epoch\n",
        "        current_best_auc = best_auc         \n",
        "\n",
        "      terminate_epoch(live_loss_plot, logs)\n",
        "\n",
        "  except KeyboardInterrupt:\n",
        "      logging.info('Training interrupted')\n",
        "      pass\n",
        "\n",
        "  log_model_summary(model, criterion, optimizer, phase_data_dirs, over_sampling, cv, False, time.time() - starting_time)\n",
        "\n",
        "  # load best model weights\n",
        "  model.load_state_dict(best_model_wts)\n",
        "\n",
        "  return model"
      ],
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "time: 91.1 ms\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OdcbQ39JXIet",
        "outputId": "7c14f0e4-633e-4518-9c5c-6abe17817acc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "def init_training(model, criterion, phase_data_dirs, num_epochs, over_sampling, cv, test_split, first_trained_module):\n",
        "  \n",
        "  preprocess_images(phase_data_dirs['train'][0])\n",
        "    \n",
        "  series_fmt = {phases_friendly_names[phase].lower():(f'{phase}_' + '{}') for phase in phase_data_dirs.keys()}\n",
        "  live_loss_plot = PlotLosses(series_fmt=series_fmt,\n",
        "                     #mark_high_score={'valid_ROC AUC': 'higher'},\n",
        "                     validation_fmt=None)\n",
        "\n",
        "  model = model.to(device)\n",
        "\n",
        "  save_path = init_log(model.__class__.__name__)\n",
        "\n",
        "  starting_time = time.time()\n",
        "\n",
        "  best_model_wts = copy.deepcopy(model.state_dict())  \n",
        "\n",
        "  phase_y = {phase: get_true_labels(folders) for (phase, folders) in phase_data_dirs.items()}\n",
        "  split_strategy = get_split_strategy(phase_y, num_epochs, cv, test_split)  \n",
        "  \n",
        "  first_trained_module_i, first_trained_module, optimizer = find_first_trained_module(model, start_module_name=first_trained_module)  \n",
        "\n",
        "  log_model_summary(model, criterion, optimizer, phase_data_dirs, over_sampling, cv, True)\n",
        "  \n",
        "  return model, optimizer, first_trained_module_i, save_path, starting_time, best_model_wts, phase_y, split_strategy, live_loss_plot"
      ],
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "time: 12 ms\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H0hjnLyhSRH7",
        "outputId": "88ee2599-dac6-4ec3-f713-2307569bd871",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "phases_friendly_names = {'train': 'Training', 'valid': 'Validation', 'test': 'Test'}"
      ],
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "time: 1 ms\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P4UdKYQYcTQv"
      },
      "source": [
        "I extracted functions to initialize, run and terminate a single epoch. So that it can be reused by test_model later."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aoxWqV3YUixl",
        "outputId": "25c0b867-3d98-49d7-ae87-172383e24350",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "def init_epoch(model, phase, folders, indices=None, phase_y=None, over_sampling=False, epoch=None, num_epochs=1, cv=None, is_test_phase=False, n_crops=None, n_crops_stages=None, best_auc=1, resize=0.75):\n",
        "  \n",
        "  if n_crops is None:\n",
        "    n_crops = get_n_crops(is_test_phase, best_auc, n_crops_stages)\n",
        "  \n",
        "  fold = '' if cv is None else f'- KFold {cv if (epoch % cv) == 0 else (epoch % cv)}/{cv} '\n",
        "  crops = '' if phase=='train' else f'{n_crops}-crop' + ('' if n_crops==1 else 's')\n",
        "  epoch = '' if num_epochs==1 else f'Epoch {epoch}/{num_epochs} '\n",
        "  logging.info(f'\\n{epoch}{fold}{phases_friendly_names[phase]} {crops}')\n",
        "  \n",
        "  return get_data_loader(phase, model, folders, indices, phase_y, over_sampling, best_auc, n_crops, resize)"
      ],
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "time: 11.9 ms\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B77cHhZ7mjOE",
        "outputId": "f44a7eed-8d98-499b-c093-fc59b828d137",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "def init_epoch_variables():\n",
        "\n",
        "  running_loss = 0.0\n",
        "  \n",
        "  # {disease: = [nb corrects, nb processed]}\n",
        "  running_accuracy = {disease:[0, 0] for disease in classes}\n",
        " \n",
        "  # true one hot labels\n",
        "  one_hot_labels = np.empty((0,3), int)\n",
        "  \n",
        "  # predicted outputs probabilities\n",
        "  outputs_probabilities = np.empty((0,3), float)\n",
        "  \n",
        "  return running_loss, running_accuracy, one_hot_labels, outputs_probabilities"
      ],
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "time: 7.46 ms\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jUPNn9UZWRcF",
        "outputId": "da3b8998-18fd-49f1-95b9-a7e19c6f9283",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "def run_epoch(phase, data_loader, model, criterion, optimizer=None, logs={}, best_auc=0.0, best_model_wts=None, save_path=None, save_phase='test'):\n",
        "  # train, validate or test the model for one epoch\n",
        "  \n",
        "  if phase == 'train':\n",
        "      model.train()  # Set model to training mode\n",
        "  else:\n",
        "      model.eval()   # Set model to evaluate mode\n",
        "      \n",
        "  # reset epoch metrics\n",
        "  running_loss, running_accuracy, one_hot_labels, outputs_probabilities = init_epoch_variables()\n",
        "\n",
        "  # Iterate over data.\n",
        "  tqdm_items = tqdm(data_loader)\n",
        "  for inputs, labels in tqdm_items:\n",
        "      inputs = inputs.to(device)\n",
        "      labels = labels.to(device)\n",
        "\n",
        "      if optimizer is not None:\n",
        "        # zero the parameter gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "      # track history if only in train\n",
        "      with torch.set_grad_enabled(phase == 'train'):\n",
        "          # forward\n",
        "          outputs, loss = batch_forward(inputs, labels, model, model.__class__.__name__, criterion, phase)\n",
        "\n",
        "          # backward + optimize only if in training phase\n",
        "          if phase == 'train':\n",
        "              loss.backward()\n",
        "              optimizer.step()\n",
        "\n",
        "          # compute and accumulate metrics          \n",
        "          running_loss, one_hot_labels, outputs_probabilities, running_accuracy \\\n",
        "            = compute_batch_metrics(inputs, labels, outputs, loss, running_loss, one_hot_labels, outputs_probabilities, running_accuracy)\n",
        "          \n",
        "          # update progress bar status\n",
        "          tqdm_items.set_postfix_str(get_running_status(phase, running_loss, running_accuracy))\n",
        "\n",
        "  epoch_loss = running_loss / len(data_loader.sampler)\n",
        "  epoch_acc = compute_accuracy(running_accuracy)\n",
        "  epoch_auc = get_roc_auc(one_hot_labels[:,[0,2]], outputs_probabilities[:,[0,2]])\n",
        "  \n",
        "  logging.info('Items processed: {}'.format([f'{disease}: {running_accuracy[disease][1]}' for disease in running_accuracy]))\n",
        "  logging.info(get_running_status(phase, running_loss, running_accuracy))\n",
        "  logging.info(f'Cat 1 ROC AUC: {epoch_auc[0]:.3f} Cat 2 ROC AUC: {epoch_auc[1]:.3f} Cat 3 ROC AUC: {epoch_auc[2]:.3f}')\n",
        "\n",
        "  logs = update_liveplot(logs, phase, epoch_loss, epoch_acc, epoch_auc)\n",
        "\n",
        "  # deep copy the model\n",
        "  if phase == save_phase and epoch_auc[2] >= best_auc:\n",
        "      best_model_wts, best_auc = save_model(save_path, model, epoch_auc[2])\n",
        "      \n",
        "  return logs, best_auc, best_model_wts, running_accuracy, outputs_probabilities"
      ],
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "time: 39.9 ms\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GRZVgyTjYyki",
        "outputId": "f1984bf6-f4e8-4936-caa6-f571a1d96983",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "def compute_accuracy(running_accuracy):\n",
        "  \n",
        "  running_accuracy_array = np.array(list(running_accuracy.values()))\n",
        "  \n",
        "  return sum(running_accuracy_array[:,0]) / sum(running_accuracy_array[:,1])"
      ],
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "time: 2.34 ms\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IB4rwc7GUued",
        "outputId": "eebf91b3-e97c-4dbd-a74f-1937bdb7318d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "def terminate_epoch(live_loss_plot, logs):\n",
        "  \n",
        "  live_loss_plot.update(logs)\n",
        "  live_loss_plot.draw()\n",
        "\n",
        "  logging.info('')"
      ],
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "time: 1.09 ms\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fIazRrfNa90Y"
      },
      "source": [
        "I also extracted a function to calculate outputs and loss for a single batch.  \n",
        "\n",
        "Note there are two special cases:\n",
        "- special case for [Inception3](https://discuss.pytorch.org/t/how-to-optimize-inception-model-with-auxiliary-classifiers/7958) while training only because it has an auxiliary output;\n",
        "- special case for multi-crop as tensor must be transformed from 5D to 4D for computation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G-Fk194a8A89",
        "outputId": "744ad24e-d39e-49c4-90d5-dcce901e1b7e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "def batch_forward(inputs, labels, model, model_name, criterion, phase):\n",
        "\n",
        "  # Get model outputs and calculate loss\n",
        "  # Special case for inception because in training it has an auxiliary output. In train\n",
        "  #   mode we calculate the loss by summing the final output and the auxiliary output\n",
        "  #   but in testing we only consider the final output.\n",
        "  if 'inception3' in model_name.lower() and phase == 'train':\n",
        "      # From https://discuss.pytorch.org/t/how-to-optimize-inception-model-with-auxiliary-classifiers/7958\n",
        "      outputs, aux_outputs = model(inputs)\n",
        "      loss1 = criterion(outputs, labels)\n",
        "      loss2 = criterion(aux_outputs, labels)\n",
        "      loss = loss1 + 0.4*loss2\n",
        "  else:\n",
        "    if len(inputs.size()) == 5:\n",
        "      # transform 5D multi-crop tensor as 4D tensor for computation\n",
        "      labels = labels.cuda(async=True)\n",
        "      inputs_var = torch.autograd.Variable(inputs, volatile=True)\n",
        "\n",
        "      bs, ncrops, c, h, w = inputs_var.size()  \n",
        "      temp_output = model(inputs_var.view(-1, c, h, w)) # fuse batch size and ncrops\n",
        "      outputs = temp_output.view(bs, ncrops, -1).mean(1) # avg over crops\n",
        "    else:\n",
        "      outputs = model(inputs)\n",
        "    \n",
        "    loss = criterion(outputs, labels)\n",
        "  \n",
        "  return outputs, loss"
      ],
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "time: 15.5 ms\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_sIFIoUMO1et"
      },
      "source": [
        "## Ready? GO!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jjqqrbmZGsIf"
      },
      "source": [
        "And now let's train the model with 5-fold cross validation.  \n",
        "Note that normally, I should have used and plotted mean validation metric every 5 epochs. But that's not what I did."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MxSxPx9xfV1N"
      },
      "source": [
        "If you want to train your own model, just uncomment the code below, run it, and take a break... a multiple hours break üò¥ üí§...  \n",
        "\n",
        "Or keep an eye on my addictive progress bar with live loss and per-class accuracy üòµüòÉ"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tFYa37iLSWGJ",
        "outputId": "3af918c1-2617-418b-c109-ed413d800684",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "def train_densenet_model():\n",
        "\n",
        "  model, image_size = initialize_model('densenet', len(classes), use_pretrained=True)\n",
        "\n",
        "  return train_model(model, criterion, phase_data_dirs_resized, over_sampling=True, cv=5, \n",
        "                     # Freeze denseblock1 and denseblock2 (feature extraction)\n",
        "                     # Train  denseblock3 and denseblock4 (fine-tuning)\n",
        "                     first_trained_module='features.denseblock3', num_epochs=100)\n",
        "\n",
        "# uncomment the code below to train it from scratch...\n",
        "# dr_densenet_model = train_densenet_model()"
      ],
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "time: 2.48 ms\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y1RGz8Yy0TQg"
      },
      "source": [
        "![title](https://github.com/sebastienlange/dermatologist-ai/blob/master/images/densenet_training_plots.png?raw=1)\n",
        "\n",
        "\n",
        "\n",
        "```\n",
        "Epoch 70/100 - KFold 5/5\n",
        "log loss:\n",
        "training   (min:    0.057, max:    0.679, cur:    0.062)\n",
        "validation (min:    0.029, max:    0.698, cur:    0.040)\n",
        "\n",
        "accuracy:\n",
        "training   (min:    0.685, max:    0.979, cur:    0.978)\n",
        "validation (min:    0.741, max:    0.991, cur:    0.983)\n",
        "\n",
        "ROC AUC:\n",
        "\n",
        "training   (min:    0.885, max:    0.999, cur:    0.999)\n",
        "validation (min:    0.918, max:    1.000, cur:    1.000)\n",
        "\n",
        "Items processed: {'melanoma': 1510, 'nevus': 1510, 'seborrheic_keratosis': 1510}\n",
        "train_loss=0.052, train_acc=0.981, melan_acc=0.979, nevus_acc=0.966, sebor_acc=0.997\n",
        "Cat 1 ROC AUC: 0.998 Cat 2 ROC AUC: 1.000 Cat 3 ROC AUC: 0.999\n",
        "\n",
        "Items processed: {'melanoma': 377, 'nevus': 377, 'seborrheic_keratosis': 377}\n",
        "valid_loss=0.059, valid_acc=0.984, melan_acc=0.958, nevus_acc=0.995, sebor_acc=1.000\n",
        "Cat 1 ROC AUC: 0.999 Cat 2 ROC AUC: 1.000 Cat 3 ROC AUC: 1.000\n",
        "```\n",
        "\n",
        "`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZT1whttddOn4"
      },
      "source": [
        "While validation metrics are excellent after 70 epochs, test metrics have more variations ! It overfits to the training/validation set. So I did not keep the very last model. But one of the previous.  \n",
        "\n",
        "I should also mention that, in the run above, I did not add too much blur, noise and color jitter in train transforms. Hence the validation metrics that reach 1.000 ROC AUC and 99.1% accuracy.  \n",
        "\n",
        "I kept these (nice) plots because it was one of my longest runs, but the model I kept is from a shorter run, with the data augmentation defined earlier in this book."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yb_P3IIXDz7i"
      },
      "source": [
        "# Evaluate the Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4xI8CR7bhya2"
      },
      "source": [
        "Inspired by the [2017 ISIC Challenge on Skin Lesion Analysis Towards Melanoma Detection](https://challenge.kitware.com/#challenge/583f126bcad3a51cc66c8d9a), the algorithm is ranked according to three separate categories:\n",
        "- Category 1: ROC AUC for Melanoma Classification\n",
        "- Category 2: ROC AUC for Melanocytic Classification\n",
        "- Category 3: Mean ROC AUC"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jc8bsUGXG8fP"
      },
      "source": [
        "The test_model function below calculates the test Mean ROC AUC of the model on the test set.  \n",
        "It also returns a data frame with the probabilities of detection of melanoma and seborrheic keratoses in the given images.  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-02-14T07:53:31.549287Z",
          "start_time": "2019-02-14T07:53:31.540343Z"
        },
        "id": "WstnWoes_V9B",
        "outputId": "fafa6898-6bbf-43d9-faa9-84e468633354",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "def test_model(images_folder, criterion, model=None, resize=0.75, n_crops=10, plot=True, model_fn=None, force_test=False):\n",
        "  \n",
        "  predictions = None\n",
        "  if model_fn is not None and not force_test:\n",
        "    predictions, accuracy, best_auc = load_results(model_fn, n_crops, plot)\n",
        "  \n",
        "  if predictions is None:  \n",
        "    preprocess_images(images_folder)\n",
        "\n",
        "    if model is None: model = load_model(model_fn)\n",
        "\n",
        "    # make batch_size inversely proportional to the crop and compatible with biggest models... otherwise CUDA out of memory ;-)\n",
        "    batch_size['test'] = int(40 / n_crops)\n",
        "\n",
        "    if len(logging.getLogger().handlers)==1:\n",
        "      init_log('test')\n",
        "\n",
        "    data_loader = init_epoch(model, 'test', [images_folder], n_crops=n_crops)\n",
        "\n",
        "    _, best_auc, _, accuracy, data = run_epoch('test', data_loader, model, criterion)\n",
        "\n",
        "    file_names = [os.path.basename(fn) for fn in sorted(glob.glob(os.path.join(images_folder, '*', '*.jpg')))]\n",
        "    predictions = pd.DataFrame(index=file_names, data=data, columns=classes)\n",
        "\n",
        "    if plot:\n",
        "      _ = get_roc_auc(y_true, predictions.as_matrix(columns=[classes[0], classes[2]]), plot=True)\n",
        "\n",
        "    save_results(model_fn, n_crops, predictions)\n",
        "  \n",
        "  return predictions, accuracy, best_auc, model\n",
        "\n",
        "plt.rcParams['figure.figsize'] = (8, 6)"
      ],
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "time: 23.6 ms\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "psYtjAVhtP1k"
      },
      "source": [
        "Thanks to functions below, test_model also save results to a csv file so that the heavy computation is done once for all... For any subsequent tests with the same model filename, the results are simply reloaded. Use force_test=True to override this behavior."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Xv_aP_BLGVf",
        "outputId": "18ac1e1c-d64a-4bd4-8ce3-e8be034b8cda",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "def save_results(model_fn, n_crops, predictions):\n",
        "  # test once for all... it takes time :-)\n",
        "  \n",
        "  path_to_save = os.path.join(google_drive_shared_path if os.path.exists(google_drive_shared_path) else os.getcwd(), 'results')\n",
        "  predictions.to_csv(os.path.join(path_to_save, model_fn.replace('.pt', f'-{n_crops}-crops.csv'))) \n",
        "  \n",
        "def load_results(model_fn, n_crops, plot):\n",
        "  \n",
        "  predictions = None\n",
        "  accuracy = None\n",
        "  best_auc = None\n",
        "  results_fn = model_fn.replace('.pt', f'-{n_crops}-crops.csv')\n",
        "\n",
        "  if os.path.exists(os.path.join(os.getcwd(), 'results', results_fn)):\n",
        "    results_fn = os.path.join(os.getcwd(), 'results', results_fn)\n",
        "  elif os.path.exists(os.path.join(google_drive_shared_path, 'results', results_fn)):\n",
        "    results_fn = os.path.join(google_drive_shared_path, 'results', results_fn)\n",
        "  else:\n",
        "    download('https://raw.githubusercontent.com/sebastienlange/dermatologist-ai/master/results/' + results_fn, os.path.join(os.getcwd(), 'results'))\n",
        "    results_fn = os.path.join(os.getcwd(), 'results', results_fn)\n",
        "\n",
        "  if os.path.exists(results_fn):\n",
        "    if plot: print(f'Loading results from {results_fn}...\\n')\n",
        "    predictions = pd.read_csv(results_fn)\n",
        "    best_auc, accuracy = compute_scores(predictions.as_matrix(columns=[classes[0], classes[1], classes[2]]), plot)\n",
        "    if plot:\n",
        "      print(f'        Accuracy: {accuracy:.3f}')\n",
        "    else:\n",
        "      print(f'Loaded results from {results_fn}: accuracy = {accuracy:.3f} - ROC AUC Cat3 = {best_auc:0.3f}')\n",
        "             \n",
        "  return predictions, accuracy, best_auc\n",
        "             \n",
        "def compute_scores(predictions, plot=False):\n",
        "\n",
        "  combined_roc_auc = get_roc_auc(y_true, predictions[:, [0,2]], plot=plot)[2]\n",
        "  combined_accuracy = sum(y == np.argmax(predictions, axis=1))/predictions.shape[0]\n",
        "  \n",
        "  return combined_roc_auc, combined_accuracy"
      ],
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "time: 26.2 ms\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4qIV7RSficUh"
      },
      "source": [
        "I need to download the ground_truth.csv where true labels are stored for melanoma and seborrheic keratoses."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "07eVrK9S5yIH",
        "outputId": "1a7353d3-90d9-4029-d697-95d4fd728f8c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "download('https://raw.githubusercontent.com/sebastienlange/dermatologist-ai/master/ground_truth.csv')"
      ],
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\rground_truth.csv      0%[                    ]       0  --.-KB/s               \rground_truth.csv    100%[===================>]  23.36K  --.-KB/s    in 0.006s  \n",
            "time: 377 ms\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J3xv30oykjI1",
        "outputId": "b0a62016-3a33-4be5-f022-32fddc1d01a4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 361
        }
      },
      "source": [
        "def get_ground_truth():\n",
        "    # get ground truth labels for test dataset\n",
        "    truth = pd.read_csv('ground_truth.csv')\n",
        "    y_true = truth.as_matrix(columns=[\"task_1\", \"task_2\"])\n",
        "    return y_true\n",
        "  \n",
        "y_true = get_ground_truth()\n",
        "y = np.argmax([[0, 1, 0] if sum(arr)==0 else arr for arr in np.insert(y_true, 1, 0, axis=1)], axis=1)"
      ],
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-78-2cebf198eefe>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0my_true\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0my_true\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_ground_truth\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;36m0\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0marr\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0marr\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minsert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-78-2cebf198eefe>\u001b[0m in \u001b[0;36mget_ground_truth\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0;31m# get ground truth labels for test dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mtruth\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'ground_truth.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0my_true\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtruth\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_matrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"task_1\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"task_2\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0my_true\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   5137\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_info_axis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_can_hold_identifiers_and_holds_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5138\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5139\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mobject\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getattribute__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5140\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5141\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__setattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'DataFrame' object has no attribute 'as_matrix'"
          ]
        },
        {
          "output_type": "stream",
          "text": [
            "time: 24.1 ms\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_zc1wXu3kLxR"
      },
      "source": [
        "Now we are ready to __evaluate__ our pretrained DenseNet model, plot __ROC AUC__ and see how it performs! Let's name it __Dr DenseNet Model__. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7B_yV1aZclKj"
      },
      "source": [
        "# uncomment force_test=True if you want to skip loading results but rather force evaluating the model\n",
        "predictions, accuracy, best_auc, dr_densenet_model = test_model(data_dir['test'] + '_resized', criterion, model_fn='DenseNet.pt',\n",
        "                                                               #force_test=True\n",
        "                                                               )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XvDjJ5RJiRg3"
      },
      "source": [
        "__0.914__! Well done!!!\n",
        "\n",
        "__Dr. DenseNet__ is an excellent Doctor, with excellent results!  If he had participated in the [2017 ISIC Challenge on Skin Lesion Analysis Towards Melanoma Detection](https://challenge.kitware.com/#challenge/583f126bcad3a51cc66c8d9a), he __would have reached the TOP 1__ with a 0.003 gain over the highest score!\n",
        "\n",
        "üìùBut it's still a human and sometimes he makes mistakes. As I am a rather anxious person, I would prefer to __ask for a second medical opinion__... By far, my biggest learning experience was: do not be too confident in one single doctor üòâ üìù"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sqxfbkoGNtEl"
      },
      "source": [
        "release_model(dr_densenet_model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1blCgMt_N9pT"
      },
      "source": [
        "# Go Further"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tMiK37imOw8-"
      },
      "source": [
        "## Second Medical Opinion"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wCL-VYvdTxSK"
      },
      "source": [
        "So let's seek for a second medical opinion from __Dr. [Inception v3](https://arxiv.org/abs/1512.00567)__.  \n",
        "\n",
        "![Inception v3](https://cdn-images-1.medium.com/max/960/1*gqKM5V-uo2sMFFPDS84yJw.png)\n",
        "\n",
        "Note that I again need to specify the layers I want to freeze (feature extraction) and the ones I want to train (fine-tuning). I choose to fine-tune from the layer named Mixed_5c."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VOdMjzfGyv5r"
      },
      "source": [
        "def train_inception3_model():\n",
        "  \n",
        "  model, image_size = initialize_model('inception3', len(classes), use_pretrained=True)\n",
        "\n",
        "  batch_size['train'] = 128\n",
        "  \n",
        "  return train_model(model, criterion, phase_data_dirs_resized, over_sampling=False, cv=5,\n",
        "                     first_trained_module='Mixed_5c', num_epochs=100, fine_tuning_module_rounds=2)\n",
        "\n",
        "# uncomment the code below to train it from scratch...\n",
        "# dr_inception_model = train_inception3_model()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J5WSpQSZDBO5"
      },
      "source": [
        "# load my highest Inception3 scores\n",
        "predictions, accuracy, best_auc, dr_inception_model = test_model(data_dir['test'] + '_resized', criterion, model_fn='Inception3_3_0.9089.pt')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rhyafe8C3ywU"
      },
      "source": [
        "Same score as the winner of the challenge for Dr Inception!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W5ysqYeXOCrR"
      },
      "source": [
        "release_model(dr_inception_model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hfoyjLaYOUXd"
      },
      "source": [
        "## Third Medical Opinion"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yzSu5-04Ul0h"
      },
      "source": [
        "I now have two medical opinions! Great! But __Dr. DenseNet and Dr. Inception sometimes gives me opposite opinion!__  \n",
        "So let's look for a third medical opinion that will be able to decide: __Dr. [NASNetALarge](https://arxiv.org/abs/1707.07012) arrives on stage!__  \n",
        "\n",
        "As stated in the [paper](https://arxiv.org/abs/1707.07012) that describes it, this is the __most accurate architecture__ as of April 2018:\n",
        "\n",
        "![NasNet performance](https://1.bp.blogspot.com/-E1qM-CKq-BA/WfuGc22fPBI/AAAAAAAACIg/frpwbO5Jh-oL0cSObyJa29fXkBsuVl7CACLcBGAs/s640/image3.jpg)\n",
        "\n",
        "Again I need to specify the layers I want to fine-tune. I \"arbitrarily\" choose to set the fine-tuning cursor at a layer named cell_4.  \n",
        "\n",
        "Ok it is the most accurate, but as the name suggests: it is large, very large. I'm not sure I'm patient enough to train it successfully, especially since I made an arbitrary choice.\n",
        "\n",
        "And last thing ... Google Colab __lacks GPU memory__ to train it worthily! I have to __reduce the train batch size to 12__."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g_lTUuuOIE4q"
      },
      "source": [
        "batch_size = {phase: 12 if phase=='train' else 2 for phase in phases}\n",
        "\n",
        "def train_nasnetalarge_model():\n",
        "  \n",
        "  model, image_size = initialize_model('nasnetalarge', len(classes), use_pretrained=True)\n",
        "\n",
        "  return train_model(model, criterion, phase_data_dirs_resized, over_sampling=True, cv=5,\n",
        "                     first_trained_module='cell_4', num_epochs=100)\n",
        "\n",
        "# uncomment the code below to train it from scratch...\n",
        "# dr_nasnetalarge_model = train_nasnetalarge_model()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WIRofR6Ea17-"
      },
      "source": [
        "I commented the code below because initializing NASNetALarge takes more time. But feel free to uncomment it!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rzd_SQnSo65w"
      },
      "source": [
        "predictions, accuracy, best_auc, dr_nasnetalarge_model = test_model(data_dir['test'] + '_resized', criterion, model_fn='NASNetALarge_4_0.9106.pt')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kDgsalqB3-j6"
      },
      "source": [
        "0.914! High score for Dr NasNetALarge!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n_63Hqly2x-0"
      },
      "source": [
        "release_model(dr_nasnetalarge_model) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uwrWs6MIC1-K"
      },
      "source": [
        "## Just another one for fun"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aKfZyzQTCzXa"
      },
      "source": [
        "Hey, I can not help but seek __Dr. Xception's__ medical opinion! üòÅ"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Cb2qoqL7oO4"
      },
      "source": [
        "def train_xception_model(model=None):\n",
        "\n",
        "  model, image_size = initialize_model('xception', len(classes), use_pretrained=True)\n",
        "\n",
        "  return train_model(model, criterion, phase_data_dirs_resized, over_sampling=True, cv=5,\n",
        "                     first_trained_module='block4', num_epochs=100, fine_tuning_module_rounds=5, \n",
        "                     n_crops_stages={5: 0.885, 10: 0.900})\n",
        "\n",
        "# uncomment the code below to train it from scratch...\n",
        "#dr_xception_model = train_xception_model()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w616WCeCJcfL"
      },
      "source": [
        "# load my highest Xception score\n",
        "predictions, accuracy, best_auc, dr_xception_model = test_model(data_dir['test'] + '_resized', criterion, n_crops=10, model_fn='Xception_2_block3.rep.4.conv1_0.9185.pt')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t1rWgfrhJyvq"
      },
      "source": [
        "0.918! Highest score for Dr Xception!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KhiY-M-NOMvW"
      },
      "source": [
        "release_model(dr_xception_model) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uy6Wx_m7FPLO"
      },
      "source": [
        "## Committee of Doctors"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nc7ovmIJQFr2"
      },
      "source": [
        "I'm lucky enough to bring together my four doctors in a committee! Alone, they all fit in the TOP 1! What a dream team! üò≤\n",
        "\n",
        "They share their results, debate, and __Dr. Inception comes to suggest looking at the dermatoscopic images from different perspectives__.  \n",
        "\n",
        "He suggest the following crops:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "odpGHYrMRh8Y"
      },
      "source": [
        "crops = [1, 5, 10, 20]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ewj0TmaaNCtf"
      },
      "source": [
        "But wait a minute ... actually I just had a __hard time stopping to learn üìù and try other ideas and other models ...__ üòÖ  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JXQ543je806D",
        "cellView": "form"
      },
      "source": [
        "#@title\n",
        "def get_unique_models(model_names):\n",
        "  \n",
        "  return sorted(set(model_names))\n",
        "\n",
        "def get_unique_models_count(model_names):\n",
        "  \n",
        "  return len(get_unique_models(model_names))\n",
        "\n",
        "trained_models = list(trained_models_files.keys())\n",
        "model_names = [get_model_name(fn) for fn in trained_models_files.keys()]\n",
        "\n",
        "HTML(f\"In the end, I kept {len(trained_models)} trials from <b>{get_unique_models_count(model_names)} unique models</b> {get_unique_models(model_names)}, each tested at {len(crops)} different crops {crops}, this makes a total of <b>{len(trained_models)*len(crops)} different perspectives</b>. It's too much.<p>Lets' choose some <b>threshold to eliminate the worst ones</b>:\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cqrcCKqo9V0U"
      },
      "source": [
        "def threshold_results(roc_auc_value, accuracy):\n",
        "  \n",
        "  accuracy = accuracy if type(accuracy)==float or type(accuracy)==np.float64 else compute_accuracy(accuracy)\n",
        "  \n",
        "  return (round(roc_auc_value,3) >= 0.907 and round(accuracy,2)>=0.75) \\\n",
        "       or round(roc_auc_value,3) > 0.911 \\\n",
        "       or round(accuracy,2) >= 0.80"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kswQ6EduVTym",
        "cellView": "form"
      },
      "source": [
        "#@title\n",
        "def test_model_with(model_fn, folder, crops, criterion):\n",
        "  # test model once per given crop\n",
        "  \n",
        "  all_predictions = {}\n",
        "  model = None\n",
        "  \n",
        "  for crop in crops:\n",
        "      \n",
        "    print(f'\\nTesting {model_fn} with {crop}-crops')\n",
        "    predictions, accuracy, best_auc, model = test_model(folder, criterion, model, plot=False, n_crops=crop, model_fn=model_fn)     \n",
        "    all_predictions[crop] = (predictions, accuracy, best_auc)\n",
        "    \n",
        "  if model is not None:\n",
        "    release_model(model)\n",
        "      \n",
        "  return all_predictions\n",
        "\n",
        "HTML(f\"Now I load my <b>{len(trained_models)} trials</b>... and test them at {len(crops)} different crops {crops}.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rTuFIFCVmoLa"
      },
      "source": [
        "As the test results of the trained models are available on my github account, \"test\" represents only the cost necessary to download .csv files and to evaluate parameters ... otherwise it takes several hours!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1A-HbGKaoGYp",
        "cellView": "form"
      },
      "source": [
        "#@title\n",
        "# test all models with given crops\n",
        "predictions = {i:test_model_with(model_fn, data_dir['test'] + '_resized', crops, criterion) for i, model_fn in enumerate(trained_models)}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bKZuxEW1w0ag"
      },
      "source": [
        "### Single-model scores"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4F96yuwwk_59"
      },
      "source": [
        "Here are their individual scores with different crops:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tFW8bljw43Ek",
        "cellView": "form"
      },
      "source": [
        "#@title\n",
        "def get_summary_row(test_result, n_crop, model_name):\n",
        "    \n",
        "  return [f'Dr. {model_name}', \n",
        "          n_crop, \n",
        "          np.round(test_result[1] if type(test_result[1])==float or type(test_result[1])==np.float64 else compute_accuracy(test_result[1]), 3), \n",
        "          np.round(test_result[2], 3)]\n",
        "\n",
        "def append_summary_row(summary, new_row, background=''):\n",
        "  \n",
        "  return summary.append({'Model': f'<b>{new_row[0]}</b>', \n",
        "                'Crop':f'{new_row[1]}', \n",
        "                'Accuracy': '<b{0}>{1:.3f}</b>'.format(background, new_row[2]),\n",
        "                'Cat 3 ROC AUC': '<b{0}>{1:.3f}</b>'.format(background, new_row[3])\n",
        "                         }, ignore_index=True)\n",
        "\n",
        "def create_summary(predictions, trained_model_names, crops):\n",
        "  \n",
        "  data = [get_summary_row(predictions[model_idx][crop], crop, trained_model_names[model_idx]) for crop in crops for model_idx in range(len(trained_models))\n",
        "         if threshold_results(predictions[model_idx][crop][2], predictions[model_idx][crop][1])]\n",
        "  summary = pd.DataFrame(data=data, columns=['Model', 'Crop', 'Accuracy', 'Cat 3 ROC AUC'])\n",
        "  summary = summary.sort_values(by=['Cat 3 ROC AUC', 'Accuracy', 'Model'], ascending=[False, False, True])\n",
        "  summary = append_summary_row(summary, ['Mean', '', summary['Accuracy'].mean(), summary['Cat 3 ROC AUC'].mean()])\n",
        "  \n",
        "  return summary\n",
        "\n",
        "summary = create_summary(predictions, trained_models, crops)\n",
        "HTML(summary.to_html(escape=False))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ue5zRpecXx1K",
        "cellView": "form"
      },
      "source": [
        "#@title\n",
        "mean_auc = summary.iloc[-1]['Cat 3 ROC AUC']\n",
        "min_auc = np.min([item for item in summary['Cat 3 ROC AUC'][:-1]])\n",
        "max_auc = np.max([item for item in summary['Cat 3 ROC AUC'][:-1]])\n",
        "HTML(f'So the chosen threshold makes it possible to select the {len(summary)-1} best persepectives out of {len(trained_models)*len(crops)} available.<p>Their individual Cat 3 ROC AUC range from <b>{min_auc} to {max_auc}</b>, with an <b>average of {mean_auc}</b>.')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o2D18qPCBht1"
      },
      "source": [
        "### Build the best team"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mDpRJfZElyzE"
      },
      "source": [
        "At this stage, my first feeling was to try a __weighted average__ to give more importance to my prefered (and most trained) models üòâ. But it was __too complex__ for a first attempt and not necessary at all!\n",
        "\n",
        "üìù Indeed, a __simple average works just fine__! So let's give equal value to each perspective and extract for each image the mean of all probabilities.üìù  \n",
        "\n",
        "However, I do not average all the trials, but I try many n-combinations from unique architectures: I __build the best team__ to achieve the __highest metrics__ while giving each architecture __at most a unique and equal voting right__."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "78vufvCsHhyp",
        "cellView": "form"
      },
      "source": [
        "#@title\n",
        "import itertools\n",
        "\n",
        "def get_combined_predictions(predictions_by_model, combination):\n",
        "  \n",
        "  sub_predictions_by_model = {(model_idx, crop):predictions_by_model[(model_idx, crop)] for (model_idx,crop) in combination }\n",
        "  \n",
        "  # for every image, I calculate the mean probability of my combinations\n",
        "  return np.mean(list(sub_predictions_by_model.values()), axis=0)\n",
        "\n",
        "def step(x, threshold=0):\n",
        "    return np.sign(np.sign(x-threshold)+1)\n",
        "\n",
        "def minimum_cost_threshold(y_true, y_pred, costs=(1, 1)):\n",
        "    '''\n",
        "    Find the decision threshold that minimizes the cost of errors.\n",
        "    \n",
        "    costs=(false_positive_cost, false_negative_cost)\n",
        "    '''\n",
        "    def error(x, costs):\n",
        "        return costs[0]*step(-x, 0.99) + costs[1]*step(x,0.99)\n",
        "    costs = np.array(costs)\n",
        "    costs = costs/np.sum(costs)\n",
        "    min_cost = np.inf\n",
        "    min_threshold = 0\n",
        "    for threshold in np.arange(0, 1, 0.001):\n",
        "        y_pred_bin = np.ceil(y_pred - threshold)\n",
        "#         cost = np.abs(y_true - y_pred_bin)*np.apply_along_axis(lambda i: costs[i[0]], 0, y_true)\n",
        "        cost = error(y_true-y_pred_bin, costs)\n",
        "        if np.mean(cost) < min_cost:\n",
        "#             print(cost[:10])\n",
        "            min_cost = np.mean(cost)\n",
        "            min_threshold = threshold\n",
        "    return min_threshold\n",
        "  \n",
        "def get_models_and_crops(combination):\n",
        "  return sorted([f'{trained_models[model_idx]} ({crop}-crops)' for (model_idx,crop) in combination])\n",
        "  \n",
        "def get_predictions_list(combination):\n",
        "  return list({(model_idx, crop):predictions_by_model[(model_idx, crop)] for (model_idx,crop) in combination }.values())\n",
        "  \n",
        "def process_combination(combination, very_best_auc, best_melanoma_f1_threshold, best_seborreric_f1_threshold, very_best_comb):\n",
        "  \n",
        "  # keep only combinations with unique architecture\n",
        "  if get_unique_models_count([get_model_name(trained_models[i]) for (i,crop) in combination]) == len(combination):\n",
        "    \n",
        "    combined_predictions = get_combined_predictions(predictions_by_model, combination)\n",
        "    combined_roc_auc, combined_accuracy = compute_scores(combined_predictions)\n",
        "    \n",
        "    auc_data.append({'ROC AUC': combined_roc_auc, 'multi-model': n, 'type': 'mean', 'crop': (combination[0][1] if len(combination)==1 else '')})        \n",
        "\n",
        "    seq = [x['ROC AUC'] for x in auc_data if x['multi-model']==n]\n",
        "    current_min = np.min(seq)\n",
        "    current_max = np.max(seq) \n",
        "\n",
        "    if combined_roc_auc == current_max or combined_roc_auc == current_min:\n",
        "      tqdm_items.set_postfix_str(f'ROC AUC min.-max.: {current_min:0.3f}-{current_max:0.3f}')\n",
        "      \n",
        "    # scores are sorted by roc_auc, melanoma_f1_threshold, seborreric_f1_threshold\n",
        "    round_combined_roc_auc = round(combined_roc_auc,3)\n",
        "    if round_combined_roc_auc >= very_best_auc:\n",
        "\n",
        "      melanoma_f1_threshold = round(minimum_cost_threshold(y_true[:, 0], combined_predictions[:, 0], costs=(1,10)),3)\n",
        "      if round_combined_roc_auc > very_best_auc or melanoma_f1_threshold >= best_melanoma_f1_threshold:\n",
        "        seborreric_f1_threshold = round(minimum_cost_threshold(y_true[:, 1], combined_predictions[:, 2], costs=(1,10)),3)\n",
        "        if melanoma_f1_threshold > best_melanoma_f1_threshold or seborreric_f1_threshold >= seborreric_f1_threshold:\n",
        "\n",
        "          if very_best_auc == 0:\n",
        "\n",
        "            print('High scores')\n",
        "            print('------|---------|---------|----------|--------------------------------------------------------------------------------------------------')\n",
        "            print(' ROC  | F1 mel. | F1 seb. | Accuracy | Models (n-crops)')\n",
        "            print(' AUC  | thresh. | thresh. |          |')\n",
        "            print('------|---------|---------|----------|--------------------------------------------------------------------------------------------------')\n",
        "\n",
        "          very_best_auc = round_combined_roc_auc\n",
        "          best_melanoma_f1_threshold = melanoma_f1_threshold\n",
        "          best_seborreric_f1_threshold = seborreric_f1_threshold\n",
        "          very_best_comb = combination\n",
        "          \n",
        "          if len(combination) > 1:\n",
        "            models_and_crops = get_models_and_crops(combination)\n",
        "            models_and_crops = ' + '.join(models_and_crops)\n",
        "            print(f'{combined_roc_auc:.3f} |  {best_melanoma_f1_threshold:.3f}  |  {best_seborreric_f1_threshold:.3f}  |  {combined_accuracy:.3f}   | {models_and_crops}')\n",
        "             \n",
        "  return very_best_auc, best_melanoma_f1_threshold, best_seborreric_f1_threshold, very_best_comb  \n",
        "\n",
        "\n",
        "print('Analyzing combinations of models...')\n",
        "\n",
        "predictions_by_model = {(model_idx, crop): predictions[model_idx][crop][0].as_matrix(columns=[classes[0], classes[1], classes[2]]) \n",
        "                        for crop in crops for model_idx in range(len(trained_models))\n",
        "                        if threshold_results(predictions[model_idx][crop][2], predictions[model_idx][crop][1])}\n",
        "\n",
        "auc_data = []\n",
        "\n",
        "auc_meaning = [classes[0], classes[2], 'mean']\n",
        "very_best_auc = 0\n",
        "very_best_melanoma_f1_threshold = 0\n",
        "very_best_seborreric_f1_threshold = 0\n",
        "very_best_comb = ()\n",
        "\n",
        "unique_fn = set([item.replace('Dr. ', '') for item in summary['Model'][:-1]])\n",
        "n_unique_architectures = get_unique_models_count([get_model_name(fn) for fn in unique_fn])\n",
        "\n",
        "try:\n",
        "  for n in np.arange(1, n_unique_architectures+1):      \n",
        "    print('\\n' + f'Combining {n} models out of {len(predictions_by_model)} from {n_unique_architectures} unique architectures...')\n",
        "\n",
        "    best_melanoma_f1_threshold = 0\n",
        "    best_seborreric_f1_threshold = 0\n",
        "    \n",
        "    combinations = list(itertools.combinations(predictions_by_model.keys(), n))\n",
        "    tqdm_items = tqdm(combinations)\n",
        "\n",
        "    for combination in tqdm_items:\n",
        "      very_best_auc, best_melanoma_f1_threshold, best_seborreric_f1_threshold, very_best_comb = process_combination(combination, very_best_auc, best_melanoma_f1_threshold, best_seborreric_f1_threshold, very_best_comb) \n",
        "\n",
        "except KeyboardInterrupt:\n",
        "    pass    \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OfmjVSdwvgwc"
      },
      "source": [
        "### Multi-model scores"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b5s0CPtBYijx",
        "cellView": "form"
      },
      "source": [
        "#@title\n",
        "def get_crops(m):\n",
        "  return m.split('(')[1].split('-')[0]\n",
        "\n",
        "def get_fn(m):\n",
        "  return m.split('(')[0].strip()\n",
        "\n",
        "def get_score(fn, crop):\n",
        "  return summary[(summary['Model']==f'Dr. {fn}') & (summary['Crop']==int(crop))].iloc[0]['Cat 3 ROC AUC']\n",
        "\n",
        "def print_team(name, note, combination, score):\n",
        "  return f'The <b>{name} reaches a score of {score:0.3f}</b> and consists of:<p><i>{note}</i><p><ol><li>' + '<li>'.join([f'<b>Dr. {get_model_name(get_fn(m))}</b> ({get_crops(m)}-crops) with an individual score of <b>{get_score(get_fn(m), get_crops(m))}</b> from trial {get_fn(m)}</li>' for m in sorted(get_models_and_crops(combination))]) + '</ol>'\n",
        "  \n",
        "HTML(print_team('very best team(*)', '(*) based on ROC AUC score only', very_best_comb, very_best_auc))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3NedP4WaiA9X",
        "cellView": "form"
      },
      "source": [
        "#@title\n",
        "print('Very best ROC AUC scores:')\n",
        "combined_predictions = get_combined_predictions(predictions_by_model, very_best_comb)\n",
        "combined_roc_auc, combined_accuracy = compute_scores(combined_predictions, True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XioIWajHmJRu"
      },
      "source": [
        "Wow!  Great!  \n",
        "\n",
        "__Mean ROC AUC score climbs to 0.944 and outperforms the first score of the initial challenge!__\n",
        "\n",
        "Editor's note: before giving a try to multiple models, I had, a long time ago, written in my conclusions: *\"I only tried DenseNet with all parameters freezed! I'm pretty sure that a combination of Resnet / Inception / VGG / DenseNet with some fine-tuned layers and a [VotingClassifier](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.VotingClassifier. html) would push the limits higher!\"*. And I was pretty right! üòÉüòÉüòÉ\n",
        "\n",
        "Indeed while I had the intuition of training multiple models early in this project, I discovered afterwards (see Comparison with State-of-the-art Approaches in [Review: Inception-v4](https://towardsdatascience.com/review-inception-v4-evolved-from-googlenet-merged-with-resnet-idea-image-classification-5e8c339d18bc)) that this was a __best practice__ called __multi-model__. And this is at the same time I discovered and implemented __multi-crop__ in test while I already had implemented __multi-scale__ but I did not keep it."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6eWXX04odSSY"
      },
      "source": [
        "However, IMHO this \"very best\" team requires a minimum cost threshold that is too low for an __optimized confusion matrix__. So in __real world__, to reach a __better compromise between ROC AUC and min. cost threshold__, I would __select the team below__ while admitting his minimum cost remains too low for what I expected:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7DfvgzYGeXYf",
        "cellView": "form"
      },
      "source": [
        "#@title\n",
        "selected_combination = ((trained_models.index('Inception3_3_0.9051.pt'),10),\n",
        "                      (trained_models.index('NASNetALarge.pt'),10),\n",
        "                      (trained_models.index('SENet_3_layer2.1_0.8988.pt'),10),\n",
        "                      (trained_models.index('Xception_1_block4_0.9120.pt'),5))\n",
        "\n",
        "selected_predictions = get_combined_predictions(predictions_by_model, selected_combination)\n",
        "selected_roc_auc, selected_accuracy = compute_scores(selected_predictions, False)\n",
        "\n",
        "HTML(print_team('selected best team(*)', '(*) based on a good compromise between ROC AUC and min. cost threshold for an optimized confusion matrix', selected_combination, selected_roc_auc))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LZrYSZtOhDlg",
        "cellView": "form"
      },
      "source": [
        "#@title\n",
        "print('Multi-model selected with a good compromise between ROC AUC and min. cost threshold for an optimized confusion matrix:')\n",
        "\n",
        "selected_roc_auc, selected_accuracy = compute_scores(selected_predictions, True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WmiVkfyjS4zM",
        "cellView": "form"
      },
      "source": [
        "#@title\n",
        "download('https://raw.githubusercontent.com/sebastienlange/dermatologist-ai/master/get_results.py')\n",
        "\n",
        "from get_results import plot_confusion_matrix\n",
        "\n",
        "def plot_best_confusion_matrix(skin_disease, y_true, y_pred, costs=(1,1)):\n",
        "    threshold = minimum_cost_threshold(y_true, y_pred, costs=costs)\n",
        "    print(f'Best threshold for {skin_disease}: {threshold:0.2f}')\n",
        "    plot_confusion_matrix(y_true, y_pred, threshold, ['benign', 'malignant'])\n",
        "    \n",
        "plot_best_confusion_matrix(classes[0], y_true[:, 0], selected_predictions[:, 0], costs=(1,10))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "onZ4D-8OS-om",
        "cellView": "form"
      },
      "source": [
        "#@title\n",
        "plot_best_confusion_matrix(classes[2], y_true[:, 1], selected_predictions[:, 2], costs=(1,10))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ElHxWFi8wKY0"
      },
      "source": [
        "### Multi-model statistics"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LLzYVmk5wX4J"
      },
      "source": [
        "Before concluding, I also found very interesting to make some statisticts on multi-crop / multi-model scores.\n",
        "\n",
        "A picture is worth a thousand words! Let's draw a box plot to show __distributions of ROC AUC with respect to number of models__:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nzs8vFVriMhM",
        "cellView": "both"
      },
      "source": [
        "#@title\n",
        "best_scores = [{'n-model': 1, 'score': summary[summary['Crop']==1].iloc[0]['Cat 3 ROC AUC'], 'description': 'my best single-crop / single-model score'},\n",
        "               {'score': 0.911, 'description': 'challenge win score'},\n",
        "               {'n-model': 1, 'score': summary.loc[0]['Cat 3 ROC AUC'], 'description': 'my best multi-crop / single-model score'},\n",
        "               {'n-model': 5, 'score': 0.938, 'description': 'my best single-crop / multi-model score'},\n",
        "               {'n-model': len(selected_combination), 'score': selected_roc_auc, 'description': 'my prefered multi-model (compromise betw. ROC AUC and min. cost threshold)'},\n",
        "               {'n-model': 6, 'score': combined_roc_auc, 'description': 'my best multi-crop / multi-model score'},\n",
        "              ]\n",
        "\n",
        "auc_df = pd.DataFrame(columns=['ROC AUC', 'multi-model', 'type'], data=auc_data)\n",
        "\n",
        "sns.set(style=\"whitegrid\", palette=\"pastel\", color_codes=True)\n",
        "ax = sns.boxplot(x=\"multi-model\", y=\"ROC AUC\", data=auc_df)\n",
        "ax.set_title('Min/max scores by multi-models with outliers')\n",
        "\n",
        "for best_score in best_scores:\n",
        "  high_score = best_score['score']\n",
        "  ax.axhline(high_score, ls='--')\n",
        "  annotation = ax.text(n_unique_architectures - 0.4, high_score, f'{high_score:0.3f}: ' + best_score['description'])\n",
        "  if 'n-model' in best_score:\n",
        "    plt.plot(best_score['n-model']-1, high_score, 'y*', markersize=15, markeredgewidth=2, markeredgecolor='k')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_slCyv8FOmbL"
      },
      "source": [
        "This is interesting to see that <b>multi-model gives me ‚âÉ3.25% return over investment</b>... whereas multi-crop \"only\" gives ‚âÉ0.6%. But still that adds up üòÉ  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wcpMEJ5cHZTS"
      },
      "source": [
        "# Conclusions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YQT24CLlHpUf"
      },
      "source": [
        "I was able to achieve a __Mean ROC AUC score of 0.944__. It would have been a __TOP 1__ in the initial challenge (see scores below). It's very satisfying for what I wanted to achieve, especially since the __winner's score is 0.911__.  üòÉ\n",
        "\n",
        "But much more than this score, I learned a lot üìùüìùüìù, sometimes the hard way üòì, and took a lot of fun. üòÖ  \n",
        "\n",
        "From the first day I reached TOP 3, I said to myself: _\"let's publish tomorrow and continue your [Deep Learning nanodegree](https://www.udacity.com/course/deep-learning-nanodegree--nd101). This is an optional mini-project and you're now in a late for the rest of the nanodegree\"_... but I just wasn't able to stop training and learning by myself. It took me six more weeks of learning üìùüìùüìùüìù and wonderful experience to go from 0.900 to 0.944 ‚ú®. Now I have to catch up on my nanodegree! "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xzw8Z6T7HmKH"
      },
      "source": [
        "![title](https://github.com/sebastienlange/dermatologist-ai/blob/master/images/cat_3.png?raw=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yfkkWgao7lVF"
      },
      "source": [
        "## Possible points of improvement\n",
        "Yes! I think it's possible to further improve the results and I still have a lot to learn! \n",
        "\n",
        "So here are possible points of improvement:\n",
        "- test loss is often much higher than training/validation loss; although it was not the measure observed during the challenge, I observed it and felt that it was showing too much overfitting to the training/validation set (same for accuracy but to a lesser extent). I need __more data__! Moreover, as already said, the minimum cost threashold for an optimized confusion matrix remains too low for what I expected;\n",
        "- more or different __data augmentations__ techniques. Particularly, I integrated [imgaug](https://github.com/aleju/imgaug) lately, so there's __a lot to try with [imgaug](https://github.com/aleju/imgaug)__! Particularly add more noise in image (like improved fake hairs, patch and add fake ruler)\n",
        "- I chose to divide more or less in two equal parts, (or at one third) the number of feature extracted and fine-tuned layers. So, __I'm pretty sure there's a better point than \"randomly\" choosing first half of layers as feature extracted and second half as fine-tuned layers__. Hence my late try to add one more fine-tuned layer to train every n epochs rounds without metrics improvement; \n",
        "- use different source of images : all the images come from the same distribution\n",
        "- try [VotingClassifier](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.VotingClassifier.html) with my multi-models, or try further with [XGBClassifier](https://xgboost.readthedocs.io/en/latest/python/python_api.html), or anything else that could do better than a simple mean.\n",
        "- like challenge's TOP 3, integrate available data about age, male/female in the prediction model...\n",
        "- give more chance to other transfer learning models, and __particularly try different parameters__: I spent most on my time with DenseNet and I kept the same parameters for others while it could be adapted for each of them...\n",
        "- fine tune learning rate and other optimizer hyper parameters to speed up gradient descent (Adam with lr 0.002 often worked with earlier versions of the model)\n",
        "- __detect the area of interest for each image through segmentation and zoom in on it__.\n",
        "- give a try to other optimizers: AdaGrad, RMSProp, Adamax, Adadelta...\n",
        "- try more with different batch_size to see how it impacts training (the final model reachs a GPU out of memory with a batch size of 64 [Google colab GPU])\n",
        "- display images where the model fails to understand why it fails and maybe improve algorithm, data augmentation...  \n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "I would also love to integrate some of the tools / algorithms I used througout this project into another framework like PyTorch, TensorFlow, Keras...\n",
        "- moving fine-tuned cursor;\n",
        "- cross validation;\n",
        "- multi-crop strategy adapted to percentage of achieved objective;\n",
        "- live plot while training;\n",
        "- even cold start with smaller set of images or data augmentation that progressively adds planned transforms\n",
        "- ..."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p1wwkrGv736F"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
